import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
import random
from urllib.parse import urljoin, quote, urlparse
import json
from datetime import datetime
import io

# Configuration
st.set_page_config(
    page_title="Scraper BUMIDOM - Archives AN", 
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üîç Scraper BUMIDOM - Archives de l'Assembl√©e Nationale")
st.markdown("Recherche et analyse des documents PDF mentionnant BUMIDOM")

class SimpleBUMIDOMScraper:
    def __init__(self):
        self.base_url = "https://www.assemblee-nationale.fr"
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
    def create_session(self):
        """Cr√©e une session HTTP avec des headers r√©alistes"""
        session = requests.Session()
        
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
        
        session.headers.update(headers)
        return session
    
    def search_google_simple(self, keyword="BUMIDOM", max_results=100):
        """Utilise Google Search pour trouver des PDF sur le site AN"""
        
        st.info(f"Recherche Google pour '{keyword}'...")
        
        all_pdf_links = []
        
        # Construction de la requ√™te Google
        query = f'site:assemblee-nationale.fr filetype:pdf "{keyword}"'
        encoded_query = quote(query)
        
        # Plusieurs pages de r√©sultats Google
        for page in range(0, 10):  # 10 pages max
            start = page * 10
            
            try:
                st.write(f"üîç Page Google {page + 1}...")
                
                # URL Google Search
                google_url = f"https://www.google.com/search?q={encoded_query}&start={start}"
                
                session = self.create_session()
                response = session.get(google_url, timeout=15)
                
                if response.status_code != 200:
                    st.warning(f"Google a retourn√© {response.status_code}")
                    break
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Chercher tous les liens dans les r√©sultats
                all_links = soup.find_all('a')
                
                for link in all_links:
                    href = link.get('href', '')
                    
                    # Nettoyer les URLs Google
                    if href.startswith('/url?q='):
                        # Extraire l'URL r√©elle du param√®tre Google
                        url_match = re.search(r'/url\?q=([^&]+)', href)
                        if url_match:
                            real_url = requests.utils.unquote(url_match.group(1))
                            
                            # V√©rifier si c'est un PDF de l'AN
                            if (real_url.endswith('.pdf') or '.pdf?' in real_url) and \
                               'assemblee-nationale.fr' in real_url and \
                               keyword.lower() in real_url.lower():
                                
                                # R√©cup√©rer le titre
                                title = link.get_text(strip=True)
                                if not title or len(title) < 5:
                                    # Chercher un titre dans les parents
                                    parent = link.find_parent(['h3', 'div'])
                                    if parent:
                                        title = parent.get_text(strip=True)
                                
                                if not title:
                                    title = real_url.split('/')[-1]
                                
                                pdf_info = {
                                    'url': real_url,
                                    'title': title[:200],
                                    'source': 'Google Search',
                                    'page': page + 1,
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                # √âviter les doublons
                                if not any(p['url'] == real_url for p in all_pdf_links):
                                    all_pdf_links.append(pdf_info)
                                    st.write(f"  ‚Üí PDF: {title[:80]}...")
                
                # Pause pour respecter Google
                time.sleep(random.uniform(2, 4))
                
                # Arr√™ter si on a assez de r√©sultats
                if len(all_pdf_links) >= max_results:
                    break
                    
            except Exception as e:
                st.warning(f"Erreur page {page + 1}: {str(e)[:100]}")
                continue
        
        return all_pdf_links[:max_results]
    
    def search_direct_archives(self, keyword="BUMIDOM"):
        """Cherche directement dans les archives connues"""
        
        st.info("Recherche dans les archives directes...")
        
        all_pdf_links = []
        
        # URLs d'archives connues pour chaque l√©gislature
        archive_urls = [
            # 5√®me l√©gislature (1973-1978) - P√©riode BUMIDOM active
            ("https://archives.assemblee-nationale.fr/5/qst/", "Questions 5√®me l√©g."),
            ("https://archives.assemblee-nationale.fr/5/cri/", "D√©bats 5√®me l√©g."),
            
            # 6√®me l√©gislature (1978-1981)
            ("https://archives.assemblee-nationale.fr/6/qst/", "Questions 6√®me l√©g."),
            ("https://archives.assemblee-nationale.fr/6/cri/", "D√©bats 6√®me l√©g."),
            
            # 4√®me l√©gislature (1968-1973)
            ("https://archives.assemblee-nationale.fr/4/qst/", "Questions 4√®me l√©g."),
            ("https://archives.assemblee-nationale.fr/4/cri/", "D√©bats 4√®me l√©g."),
            
            # 7√®me l√©gislature (1981-1986)
            ("https://archives.assemblee-nationale.fr/7/qst/", "Questions 7√®me l√©g."),
            ("https://archives.assemblee-nationale.fr/7/cri/", "D√©bats 7√®me l√©g."),
        ]
        
        session = self.create_session()
        
        for url, description in archive_urls:
            try:
                st.write(f"üìÇ {description}...")
                
                response = session.get(url, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher tous les liens PDF
                    pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
                    
                    for link in pdf_links[:50]:  # Limiter √† 50 par page
                        href = link.get('href', '')
                        
                        if href:
                            # Compl√©ter l'URL si n√©cessaire
                            if not href.startswith('http'):
                                href = urljoin(url, href)
                            
                            # V√©rifier si l'URL contient le mot-cl√© ou semble pertinente
                            title = link.get_text(strip=True)
                            
                            # V√©rifier dans le titre ou l'URL
                            if (keyword.lower() in title.lower() or 
                                keyword.lower() in href.lower() or
                                'bumidom' in title.lower() or
                                'bumidom' in href.lower()):
                                
                                pdf_info = {
                                    'url': href,
                                    'title': title[:200] if title else href.split('/')[-1],
                                    'source': description,
                                    'page': 1,
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                if not any(p['url'] == href for p in all_pdf_links):
                                    all_pdf_links.append(pdf_info)
                                    st.write(f"  ‚Üí Archive: {title[:80]}...")
                
                time.sleep(1)  # Pause entre les pages
                
            except Exception as e:
                st.warning(f"Erreur {description}: {str(e)[:100]}")
                continue
        
        return all_pdf_links
    
    def search_gallica_bnf(self, keyword="BUMIDOM"):
        """Cherche dans Gallica BnF (Journal Officiel)"""
        
        st.info("Recherche dans Gallica BnF (Journal Officiel)...")
        
        all_pdf_links = []
        
        # Gallica BnF - Journal Officiel des ann√©es BUMIDOM
        for year in range(1963, 1983):  # 1963-1982
            try:
                st.write(f"üìÖ {year}...")
                
                # URL de recherche Gallica
                query = f'"{keyword}" "Journal Officiel" {year}'
                encoded_query = quote(query)
                
                gallica_url = f"https://gallica.bnf.fr/services/engine/search/sru?operation=searchRetrieve&query={encoded_query}&version=1.2"
                
                session = self.create_session()
                response = session.get(gallica_url, timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher les liens dans la r√©ponse SRU
                    for link in soup.find_all('uri'):
                        uri = link.get_text(strip=True)
                        if uri and '.pdf' in uri:
                            pdf_info = {
                                'url': uri,
                                'title': f"Journal Officiel {year} - {keyword}",
                                'source': f"Gallica BnF {year}",
                                'page': 1,
                                'timestamp': datetime.now().isoformat()
                            }
                            
                            if not any(p['url'] == uri for p in all_pdf_links):
                                all_pdf_links.append(pdf_info)
                                st.write(f"  ‚Üí Gallica: Journal Officiel {year}")
                
                time.sleep(1)
                
            except Exception as e:
                st.warning(f"Erreur ann√©e {year}: {str(e)[:100]}")
                continue
        
        return all_pdf_links
    
    def check_pdf_content(self, pdf_url, keyword="BUMIDOM"):
        """V√©rifie si un PDF contient le mot-cl√© (m√©thode simple)"""
        try:
            session = self.create_session()
            
            # T√©l√©charger seulement les premiers Ko pour v√©rifier
            headers = {'Range': 'bytes=0-100000'}  # Premier 100KB
            response = session.get(pdf_url, headers=headers, timeout=15, stream=True)
            
            if response.status_code in [200, 206]:  # 206 = Partial Content
                # Lire le contenu
                content = response.content
                
                # Convertir en texte (m√©thode simple pour les PDF textuels)
                try:
                    # Essayer de d√©coder en UTF-8
                    text = content.decode('utf-8', errors='ignore')
                    
                    # Rechercher le mot-cl√©
                    if keyword.lower() in text.lower():
                        # Compter les occurrences
                        occurrences = text.lower().count(keyword.lower())
                        
                        # Extraire un extrait
                        start_pos = text.lower().find(keyword.lower())
                        if start_pos != -1:
                            excerpt_start = max(0, start_pos - 100)
                            excerpt_end = min(len(text), start_pos + len(keyword) + 100)
                            excerpt = text[excerpt_start:excerpt_end].replace('\n', ' ').strip()
                        else:
                            excerpt = ""
                        
                        return {
                            'contains_keyword': True,
                            'occurrences': occurrences,
                            'excerpt': excerpt,
                            'error': None
                        }
                    else:
                        return {
                            'contains_keyword': False,
                            'occurrences': 0,
                            'excerpt': "",
                            'error': None
                        }
                        
                except:
                    # PDF binaire ou encod√© diff√©remment
                    return {
                        'contains_keyword': None,  # Inconnu
                        'occurrences': 0,
                        'excerpt': "",
                        'error': 'PDF binaire (OCR n√©cessaire)'
                    }
            else:
                return {
                    'contains_keyword': False,
                    'occurrences': 0,
                    'excerpt': "",
                    'error': f'HTTP {response.status_code}'
                }
                
        except Exception as e:
            return {
                'contains_keyword': False,
                'occurrences': 0,
                'excerpt': "",
                'error': str(e)[:100]
            }
    
    def multi_search(self, keyword="BUMIDOM", max_results=100):
        """Combine plusieurs m√©thodes de recherche"""
        
        st.info(f"Lancement de la recherche multi-sources pour '{keyword}'...")
        
        all_pdf_links = []
        
        # M√©thodes de recherche
        methods = [
            ("Google Search", self.search_google_simple),
            ("Archives directes", self.search_direct_archives),
            ("Gallica BnF", self.search_gallica_bnf),
        ]
        
        progress_bar = st.progress(0)
        
        for idx, (method_name, method_func) in enumerate(methods):
            progress = (idx + 1) / len(methods)
            progress_bar.progress(progress)
            
            st.write(f"üîç {method_name}...")
            
            try:
                results = method_func(keyword)
                
                # Fusionner les r√©sultats
                for pdf in results:
                    if not any(p['url'] == pdf['url'] for p in all_pdf_links):
                        all_pdf_links.append(pdf)
                
                st.success(f"  ‚Üí {len(results)} PDF trouv√©s")
                time.sleep(1)
                
            except Exception as e:
                st.warning(f"  ‚Üí {method_name} √©chou√©: {str(e)[:100]}")
        
        progress_bar.empty()
        
        return all_pdf_links[:max_results]

def main():
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        keyword = st.text_input("Mot-cl√© de recherche:", value="BUMIDOM")
        
        search_method = st.selectbox(
            "M√©thode de recherche:",
            [
                "Multi-sources (recommand√©)",
                "Google Search uniquement",
                "Archives directes",
                "Gallica BnF",
                "Test rapide"
            ]
        )
        
        max_results = st.slider("R√©sultats maximum:", 10, 200, 50)
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        
        with col1:
            search_btn = st.button("üîç Rechercher PDF", use_container_width=True)
        
        with col2:
            analyze_btn = st.button("üî¨ V√©rifier contenu", type="primary", use_container_width=True)
        
        st.markdown("---")
        st.info("""
        **Sources disponibles:**
        1. Google Search
        2. Archives AN directes
        3. Gallica BnF (JO)
        
        **P√©riode cible:** 1963-1982
        """)
    
    # Initialisation
    scraper = SimpleBUMIDOMScraper()
    
    # √âtat de session
    if 'pdf_links' not in st.session_state:
        st.session_state.pdf_links = []
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = []
    
    # Actions
    if search_btn:
        with st.spinner("Recherche en cours..."):
            if search_method == "Multi-sources (recommand√©)":
                pdf_links = scraper.multi_search(keyword, max_results)
            elif search_method == "Google Search uniquement":
                pdf_links = scraper.search_google_simple(keyword, max_results)
            elif search_method == "Archives directes":
                pdf_links = scraper.search_direct_archives(keyword)
            elif search_method == "Gallica BnF":
                pdf_links = scraper.search_gallica_bnf()
            else:
                # Test rapide
                pdf_links = scraper.search_direct_archives(keyword)[:10]
            
            st.session_state.pdf_links = pdf_links
            
            if pdf_links:
                st.success(f"‚úÖ {len(pdf_links)} PDF trouv√©s")
                
                # Afficher les r√©sultats
                df = pd.DataFrame(pdf_links)
                
                # Statistiques
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("PDF trouv√©s", len(df))
                with col2:
                    sources = df['source'].nunique()
                    st.metric("Sources", sources)
                with col3:
                    unique_urls = df['url'].nunique()
                    st.metric("URLs uniques", unique_urls)
                
                # Table des r√©sultats
                st.subheader("üìã Liste des PDF trouv√©s")
                
                for idx, pdf in enumerate(pdf_links):
                    with st.expander(f"{idx+1}. {pdf['title'][:80]}..."):
                        st.markdown(f"**URL:** `{pdf['url']}`")
                        st.markdown(f"**Source:** {pdf['source']}")
                        st.markdown(f"[üîó Ouvrir le PDF]({pdf['url']})", unsafe_allow_html=True)
                
                # Export
                st.subheader("üíæ Export")
                csv_data = df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="üì• T√©l√©charger la liste",
                    data=csv_data,
                    file_name=f"bumidom_urls_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )
                
            else:
                st.warning("‚ùå Aucun PDF trouv√©")
                
                with st.expander("üí° Conseils de recherche"):
                    st.markdown("""
                    **Pour trouver des documents BUMIDOM:**
                    
                    1. **Recherchez manuellement sur:**
                       - [Archives AN - 5√®me l√©gislature](https://archives.assemblee-nationale.fr/5/qst/)
                       - [Archives AN - 6√®me l√©gislature](https://archives.assemblee-nationale.fr/6/qst/)
                       - [Gallica BnF](https://gallica.bnf.fr)
                    
                    2. **Termes alternatifs:**
                       - "Bureau des migrations"
                       - "Migration outre-mer"
                       - "DOM TOM migration"
                       - "D√©partements d'outre-mer"
                    
                    3. **P√©riodes cl√©s:**
                       - 1973-1978 (5√®me l√©gislature)
                       - 1978-1981 (6√®me l√©gislature)
                    """)
    
    elif analyze_btn:
        if not st.session_state.pdf_links:
            st.warning("Veuillez d'abord rechercher des PDF")
        else:
            with st.spinner(f"V√©rification du contenu pour {len(st.session_state.pdf_links[:max_results])} PDF..."):
                results = []
                
                for pdf_info in st.session_state.pdf_links[:max_results]:
                    st.write(f"üîé V√©rification: {pdf_info['title'][:50]}...")
                    
                    analysis = scraper.check_pdf_content(pdf_info['url'], keyword)
                    
                    result = {
                        **pdf_info,
                        **analysis
                    }
                    
                    results.append(result)
                    
                    if analysis['contains_keyword']:
                        st.success(f"  ‚Üí Contient '{keyword}' ({analysis['occurrences']} occ.)")
                    elif analysis['contains_keyword'] is None:
                        st.info("  ‚Üí PDF binaire (n√©cessite OCR)")
                    else:
                        st.write("  ‚Üí Ne contient pas le mot-cl√©")
                
                st.session_state.analysis_results = results
                
                # Filtrer les PDF avec le mot-cl√©
                pdfs_with_keyword = [r for r in results if r.get('contains_keyword')]
                
                if pdfs_with_keyword:
                    st.success(f"‚úÖ {len(pdfs_with_keyword)} PDF contiennent '{keyword}'")
                    
                    st.subheader("üìã PDF contenant BUMIDOM")
                    
                    for pdf in pdfs_with_keyword:
                        with st.expander(f"üìÑ {pdf['title'][:80]}... ({pdf['occurrences']} occ.)"):
                            col_a, col_b = st.columns([3, 1])
                            
                            with col_a:
                                st.markdown(f"**URL:** `{pdf['url']}`")
                                st.markdown(f"**Source:** {pdf['source']}")
                                st.markdown(f"**Occurrences:** {pdf['occurrences']}")
                                
                                if pdf.get('excerpt'):
                                    st.markdown("**Extrait:**")
                                    highlighted = re.sub(
                                        r'(' + re.escape(keyword) + ')',
                                        r'**\1**',
                                        pdf['excerpt'],
                                        flags=re.IGNORECASE
                                    )
                                    st.markdown(f"> {highlighted}")
                            
                            with col_b:
                                st.markdown(f"[üåê Ouvrir PDF]({pdf['url']})", unsafe_allow_html=True)
                    
                    # Export des analyses
                    st.subheader("üíæ Export des analyses")
                    df_analysis = pd.DataFrame(pdfs_with_keyword)
                    csv_analysis = df_analysis.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="üì• T√©l√©charger analyses",
                        data=csv_analysis,
                        file_name=f"bumidom_analysis_{datetime.now().strftime('%Y%m%d')}.csv",
                        mime="text/csv"
                    )
                    
                else:
                    st.warning(f"‚ùå Aucun des PDF analys√©s ne contient '{keyword}'")
                    
                    # Afficher quand m√™me les r√©sultats
                    if results:
                        st.subheader("üìä R√©sultats d'analyse")
                        df_all = pd.DataFrame(results)
                        st.dataframe(df_all[['title', 'contains_keyword', 'occurrences', 'error']])
    
    else:
        # √âcran d'accueil
        st.markdown("""
        ## üìã Scraper BUMIDOM - Archives AN
        
        Ce dashboard recherche des documents PDF mentionnant **BUMIDOM** dans les archives parlementaires.
        
        ### üéØ P√©riode cible: 1963-1982
        - **Cr√©ation BUMIDOM:** 1963
        - **Activit√© principale:** 1963-1982
        - **Sources principales:** Questions √©crites, d√©bats parlementaires
        
        ### üîç M√©thodes de recherche:
        
        **1. Google Search**
        - Recherche: `site:assemblee-nationale.fr filetype:pdf "BUMIDOM"`
        - Avantage: Index complet de Google
        - Limite: Peut manquer des documents non index√©s
        
        **2. Archives directes**
        - Acc√®de directement aux URLs connues
        - L√©gislatures 4 √† 7 (1968-1986)
        - Questions √©crites et d√©bats
        
        **3. Gallica BnF**
        - Journal Officiel historique
        - Archives compl√®tes 1963-1982
        - PDF parfois scann√©s (OCR n√©cessaire)
        
        ### üöÄ Comment utiliser:
        
        1. **Cliquez sur "üîç Rechercher PDF"** (Multi-sources recommand√©)
        2. **Puis sur "üî¨ V√©rifier contenu"** pour analyser les PDF
        3. **Exportez** les r√©sultats en CSV
        
        ### ‚ö†Ô∏è Notes importantes:
        
        - Certains PDF sont scann√©s (n√©cessitent OCR)
        - La recherche peut prendre 1-2 minutes
        - Respectez les limites de requ√™tes
        """)

# Installation requirements
st.sidebar.markdown("---")
st.sidebar.markdown("### üì¶ Installation")
st.sidebar.code("""
pip install streamlit requests beautifulsoup4 pandas
""")

if __name__ == "__main__":
    main()
