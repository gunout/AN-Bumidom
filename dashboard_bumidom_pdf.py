# bumidom_dashboard.py
import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from io import BytesIO
import fitz  # PyMuPDF
import time
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
import hashlib
import pickle
import os
import urllib.parse
from datetime import datetime
import json

# ==================== CONFIGURATION INITIALE ====================

# Configuration de la page
st.set_page_config(
    page_title="Dashboard Analyse BUMIDOM",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# T√©l√©charger les ressources NLTK
try:
    nltk.data.find('tokenizers/punkt_tab')
    nltk.data.find('corpora/stopwords')
except:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)

# Titre de l'application
st.title("üìä Dashboard d'Analyse BUMIDOM - Archives Assembl√©e Nationale")
st.markdown("""
Analyse des documents parlementaires relatifs au **BUMIDOM** (Bureau pour le d√©veloppement 
des migrations dans les d√©partements d'outre-mer, 1963-1982).
""")

# ==================== CLASSES ET FONCTIONS UTILITAIRES ====================

class DocumentCache:
    """Cache pour stocker les PDFs t√©l√©charg√©s"""
    
    def __init__(self, cache_dir="pdf_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cache_key(self, url):
        """G√©n√®re une cl√© unique pour une URL"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def get_cached_text(self, url):
        """R√©cup√®re le texte depuis le cache"""
        cache_key = self.get_cache_key(url)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    return data.get('text', ''), data.get('pages', 0)
            except:
                return None, 0
        return None, 0
    
    def cache_text(self, url, text, pages):
        """Stocke le texte dans le cache"""
        cache_key = self.get_cache_key(url)
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump({'text': text, 'pages': pages, 'url': url, 'timestamp': time.time()}, f)
        except Exception as e:
            st.warning(f"Erreur cache: {str(e)}")

# Initialisation du cache
document_cache = DocumentCache()

# ==================== FONCTIONS DE SCRAPING R√âEL ====================

def search_bumidom_documents_real():
    """
    Scrape r√©el du site des archives pour trouver les documents BUMIDOM
    """
    base_url = "https://archives.assemblee-nationale.fr"
    search_url = f"{base_url}/r/1/search?q=BUMIDOM"
    
    documents = []
    
    try:
        st.info("üîç Scraping du site des archives en cours...")
        
        # Headers pour simuler un navigateur
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # Requ√™te HTTP
        response = requests.get(search_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # Parser le HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Plusieurs strat√©gies pour trouver les documents
        found_documents = []
        
        # Strat√©gie 1: Chercher tous les liens PDF
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
        
        # Strat√©gie 2: Chercher les r√©sultats de recherche
        search_results = soup.find_all(['div', 'li'], class_=re.compile(r'(result|item|document)', re.I))
        
        # Strat√©gie 3: Chercher par texte
        bumidom_elements = soup.find_all(text=re.compile(r'bumidom', re.I))
        
        st.info(f"PDF trouv√©s: {len(pdf_links)}, R√©sultats: {len(search_results)}, √âl√©ments BUMIDOM: {len(bumidom_elements)}")
        
        # Combiner toutes les sources
        all_elements = []
        
        # Ajouter les liens PDF directs
        for link in pdf_links[:50]:  # Limiter pour les tests
            title = link.get_text(strip=True) or link.get('href', 'Document PDF')
            url = link.get('href', '')
            
            if not url.startswith('http'):
                url = urllib.parse.urljoin(base_url, url)
            
            all_elements.append({
                'title': title,
                'url': url,
                'element': link
            })
        
        # Traiter les r√©sultats de recherche
        for result in search_results[:30]:
            # Essayer d'extraire un titre
            title_elem = result.find(['h3', 'h4', 'a', 'strong'])
            title = title_elem.get_text(strip=True) if title_elem else "Document sans titre"
            
            # Chercher un lien PDF dans ce r√©sultat
            pdf_link = result.find('a', href=re.compile(r'\.pdf$', re.I))
            if pdf_link:
                url = pdf_link.get('href', '')
                if not url.startswith('http'):
                    url = urllib.parse.urljoin(base_url, url)
                
                all_elements.append({
                    'title': title,
                    'url': url,
                    'element': result
                })
        
        # Filtrer et organiser les documents
        seen_urls = set()
        for elem in all_elements:
            url = elem['url']
            
            # √âviter les doublons
            if url in seen_urls or not url:
                continue
            
            seen_urls.add(url)
            
            # Extraire la date si possible
            date_match = re.search(r'(19\d{2}|20\d{2})', elem['title'])
            date = date_match.group(1) if date_match else "ND"
            
            # D√©terminer le type de document
            doc_type = "Document"
            title_lower = elem['title'].lower()
            
            type_patterns = [
                ('rapport', 'Rapport'),
                ('compte rendu', 'Compte rendu'),
                ('audition', 'Audition'),
                ('d√©bat', 'D√©bats'),
                ('budget', 'Budget'),
                ('question', 'Question'),
                ('loi', 'Loi'),
                ('d√©lib√©ration', 'D√©lib√©ration'),
                ('arr√™t√©', 'Arr√™t√©')
            ]
            
            for pattern, doc_type_name in type_patterns:
                if pattern in title_lower:
                    doc_type = doc_type_name
                    break
            
            documents.append({
                'title': elem['title'][:200],  # Limiter la longueur
                'url': url,
                'date': date,
                'type': doc_type,
                'pages': 0,  # Sera mis √† jour lors de l'extraction
                'source': 'Archives AN'
            })
        
        # Si peu de documents trouv√©s, en ajouter des simul√©s pour la d√©mo
        if len(documents) < 5:
            st.warning("Peu de documents trouv√©s. Ajout de documents de d√©monstration...")
            documents.extend(get_sample_documents()[:10])
        
        st.success(f"‚úÖ {len(documents)} documents trouv√©s pour analyse")
        return documents[:100]  # Limiter √† 100 documents max
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du scraping: {str(e)}")
        # Retourner des documents de d√©mo en cas d'√©chec
        return get_sample_documents()[:20]

def get_sample_documents():
    """Documents de d√©monstration si le scraping √©choue"""
    base_url = "https://archives.assemblee-nationale.fr"
    
    sample_docs = [
        {
            "title": "Rapport sur les activit√©s du BUMIDOM 1963-1965",
            "url": f"{base_url}/documents/example1.pdf",
            "date": "1966",
            "type": "Rapport",
            "pages": 45,
            "source": "D√©mo"
        },
        {
            "title": "Audition du directeur du BUMIDOM - Commission des affaires culturelles",
            "url": f"{base_url}/documents/example2.pdf",
            "date": "1970",
            "type": "Compte rendu",
            "pages": 28,
            "source": "D√©mo"
        },
        {
            "title": "Bilan des migrations DOM-TOM organis√©es par le BUMIDOM 1963-1977",
            "url": f"{base_url}/documents/example3.pdf",
            "date": "1978",
            "type": "Bilan",
            "pages": 62,
            "source": "D√©mo"
        },
        {
            "title": "Questions au gouvernement concernant le BUMIDOM",
            "url": f"{base_url}/documents/example4.pdf",
            "date": "1975",
            "type": "Question √©crite",
            "pages": 12,
            "source": "D√©mo"
        },
        {
            "title": "D√©bats parlementaires sur le financement du BUMIDOM",
            "url": f"{base_url}/documents/example5.pdf",
            "date": "1972",
            "type": "D√©bats",
            "pages": 35,
            "source": "D√©mo"
        },
        {
            "title": "Rapport d'enqu√™te sur les conditions d'accueil des migrants du BUMIDOM",
            "url": f"{base_url}/documents/example6.pdf",
            "date": "1980",
            "type": "Rapport d'enqu√™te",
            "pages": 78,
            "source": "D√©mo"
        },
        {
            "title": "Statistiques des migrations BUMIDOM 1963-1981",
            "url": f"{base_url}/documents/example7.pdf",
            "date": "1982",
            "type": "Statistiques",
            "pages": 54,
            "source": "D√©mo"
        },
        {
            "title": "Projet de loi de finances - Budget BUMIDOM 1974",
            "url": f"{base_url}/documents/example8.pdf",
            "date": "1974",
            "type": "Budget",
            "pages": 42,
            "source": "D√©mo"
        }
    ]
    
    # Ajouter plus de documents vari√©s
    for i in range(9, 21):
        year = 1963 + (i * 2) % 20
        sample_docs.append({
            "title": f"Document BUMIDOM {i} - Analyse {year}",
            "url": f"{base_url}/documents/example{i}.pdf",
            "date": str(year),
            "type": ["Rapport", "Note", "√âtude", "Communication"][i % 4],
            "pages": 20 + (i * 3) % 40,
            "source": "D√©mo"
        })
    
    return sample_docs

# ==================== FONCTIONS D'EXTRACTION PDF ====================

def extract_text_from_pdf_real(pdf_url, max_pages=50):
    """
    Extrait le texte d'un PDF r√©el depuis une URL
    """
    # V√©rifier le cache d'abord
    cached_text, cached_pages = document_cache.get_cached_text(pdf_url)
    if cached_text is not None:
        return cached_text, cached_pages
    
    try:
        # Headers pour la requ√™te
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/pdf, */*',
            'Referer': 'https://archives.assemblee-nationale.fr/'
        }
        
        # T√©l√©charger le PDF
        response = requests.get(pdf_url, headers=headers, timeout=60, stream=True)
        response.raise_for_status()
        
        # V√©rifier le type de contenu
        content_type = response.headers.get('content-type', '')
        is_pdf = 'pdf' in content_type.lower() or response.content[:4] == b'%PDF'
        
        if not is_pdf:
            # Essayer de lire quand m√™me
            st.warning(f"Type de contenu suspect pour {pdf_url}: {content_type}")
        
        # Ouvrir le PDF avec PyMuPDF
        pdf_document = fitz.open(stream=response.content, filetype="pdf")
        
        # Extraire le texte page par page
        text = ""
        total_pages = pdf_document.page_count
        
        # Limiter le nombre de pages pour les gros documents
        pages_to_extract = min(max_pages, total_pages)
        
        for page_num in range(pages_to_extract):
            page = pdf_document.load_page(page_num)
            page_text = page.get_text("text")
            
            if page_text:
                # Nettoyer le texte
                page_text = re.sub(r'\s+', ' ', page_text)
                page_text = re.sub(r'\n\s*\n', '\n\n', page_text)
                text += f"--- Page {page_num + 1} ---\n{page_text}\n\n"
        
        pdf_document.close()
        
        # Mettre en cache
        document_cache.cache_text(pdf_url, text, total_pages)
        
        return text, total_pages
        
    except requests.exceptions.RequestException as e:
        error_msg = f"Erreur r√©seau: {str(e)}"
        st.warning(f"‚ùå {error_msg} pour {pdf_url}")
        return error_msg, 0
    except fitz.FileDataError as e:
        error_msg = f"Fichier PDF invalide: {str(e)}"
        return error_msg, 0
    except Exception as e:
        error_msg = f"Erreur d'extraction: {str(e)}"
        return error_msg, 0

# ==================== FONCTIONS D'ANALYSE ====================

def analyze_document_text(text):
    """Analyse le texte d'un document"""
    if not isinstance(text, str) or len(text.strip()) < 10:
        return {
            'word_count': 0,
            'keyword_counts': {},
            'top_words': [],
            'themes': {},
            'is_valid': False
        }
    
    try:
        # Compter les mots
        words = text.split()
        word_count = len(words)
        
        # Mots-cl√©s sp√©cifiques BUMIDOM
        keywords = [
            "BUMIDOM", "bumidom", "migration", "migrant", "migrants",
            "outre-mer", "DOM", "TOM", "d√©partement", "d√©partements",
            "Guadeloupe", "Martinique", "R√©union", "Guyane", "Mayotte",
            "emploi", "travail", "ch√¥mage", "int√©gration", "accueil",
            "organis√©", "d√©veloppement", "bureau", "politique", "m√©tropole",
            "transport", "logement", "sant√©", "√©ducation", "formation",
            "famille", "jeunes", "contrat", "statistique", "bilan"
        ]
        
        # Compter les occurrences
        keyword_counts = {}
        text_lower = text.lower()
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            # Utiliser regex pour des mots complets
            pattern = r'\b' + re.escape(keyword_lower) + r'\b'
            count = len(re.findall(pattern, text_lower))
            if count > 0:
                keyword_counts[keyword] = count
        
        # Mots les plus fr√©quents (hors mots vides)
        try:
            stop_words = set(stopwords.words('french'))
            # Ajouter des mots vides courants
            stop_words.update(['plus', 'tout', 'tous', 'toutes', 'comme', 'faire', 'tr√®s'])
            
            tokens = word_tokenize(text_lower)
            filtered_tokens = [
                word for word in tokens 
                if word.isalnum() 
                and word not in stop_words 
                and len(word) > 2
                and not any(char.isdigit() for char in word)
            ]
            
            word_freq = Counter(filtered_tokens)
            top_words = word_freq.most_common(10)
        except:
            top_words = []
        
        # Identifier les th√®mes
        themes = {
            "Migration": ["migration", "migrant", "d√©part", "arriv√©e", "d√©placement", "voyage"],
            "Territoires": ["guadeloupe", "martinique", "r√©union", "guyane", "mayotte", "dom", "tom"],
            "Emploi": ["emploi", "travail", "ch√¥mage", "qualification", "formation", "m√©tier"],
            "Politique": ["politique", "gouvernement", "minist√®re", "budget", "loi", "d√©cision"],
            "Social": ["int√©gration", "accueil", "logement", "famille", "sant√©", "√©ducation"],
            "Administratif": ["bureau", "administration", "service", "direction", "organisation"]
        }
        
        theme_counts = {}
        for theme, theme_keywords in themes.items():
            total = 0
            for keyword in theme_keywords:
                pattern = r'\b' + re.escape(keyword) + r'\b'
                total += len(re.findall(pattern, text_lower))
            if total > 0:
                theme_counts[theme] = total
        
        return {
            'word_count': word_count,
            'keyword_counts': keyword_counts,
            'top_words': top_words,
            'themes': theme_counts,
            'is_valid': True
        }
        
    except Exception as e:
        st.warning(f"Erreur analyse texte: {str(e)}")
        return {
            'word_count': 0,
            'keyword_counts': {},
            'top_words': [],
            'themes': {},
            'is_valid': False
        }

def analyze_all_documents(documents):
    """Analyse tous les documents"""
    st.info(f"üî¨ Analyse de {len(documents)} documents en cours...")
    
    all_analyses = []
    all_stats = []
    all_text = ""
    corpus_word_freq = Counter()
    theme_frequencies = Counter()
    
    # Barre de progression
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Initialiser les compteurs de th√®mes
    theme_categories = ["Migration", "Territoires", "Emploi", "Politique", "Social", "Administratif"]
    for theme in theme_categories:
        theme_frequencies[theme] = 0
    
    for idx, doc in enumerate(documents):
        # Mise √† jour de la progression
        progress = (idx + 1) / len(documents)
        progress_bar.progress(progress)
        status_text.text(f"üìÑ Document {idx+1}/{len(documents)}: {doc['title'][:50]}...")
        
        # Extraire le texte
        text, pages = extract_text_from_pdf_real(doc['url'])
        
        # Mettre √† jour le nombre de pages
        doc['pages'] = pages
        
        # Analyser le texte
        analysis = analyze_document_text(text)
        
        # Pr√©parer l'analyse du document
        doc_analysis = {
            'title': doc['title'],
            'date': doc['date'],
            'type': doc['type'],
            'pages': pages,
            'url': doc['url'],
            'source': doc.get('source', 'Archive'),
            'word_count': analysis['word_count'],
            'keyword_counts': analysis['keyword_counts'],
            'top_words': analysis['top_words'],
            'themes': analysis['themes'],
            'is_valid': analysis['is_valid'],
            'text_preview': text[:500] + "..." if isinstance(text, str) and len(text) > 500 else text
        }
        
        all_analyses.append(doc_analysis)
        
        # Ajouter aux statistiques
        all_stats.append({
            'Titre': doc['title'][:80],
            'Ann√©e': doc['date'],
            'Type': doc['type'],
            'Pages': pages,
            'Mots': analysis['word_count'],
            'Fr√©q. BUMIDOM': analysis['keyword_counts'].get('BUMIDOM', 0),
            'Migrants': analysis['keyword_counts'].get('migrant', 0) + analysis['keyword_counts'].get('migrants', 0),
            'Valide': "‚úÖ" if analysis['is_valid'] else "‚ùå"
        })
        
        # Ajouter au corpus global
        if isinstance(text, str) and analysis['is_valid']:
            all_text += text + " "
            
            # Ajouter aux fr√©quences de mots du corpus
            try:
                stop_words = set(stopwords.words('french'))
                tokens = word_tokenize(text.lower())
                filtered_tokens = [
                    word for word in tokens 
                    if word.isalnum() 
                    and word not in stop_words 
                    and len(word) > 3
                ]
                corpus_word_freq.update(filtered_tokens)
            except:
                pass
            
            # Ajouter aux th√®mes
            for theme, count in analysis['themes'].items():
                theme_frequencies[theme] += count
        
        # Petite pause pour ne pas surcharger
        time.sleep(0.05)
    
    # Analyser le corpus complet
    corpus_analysis = {}
    if all_text:
        # Th√®mes principaux dans tout le corpus
        corpus_analysis['theme_frequencies'] = dict(theme_frequencies)
        
        # Mots les plus fr√©quents
        corpus_analysis['top_corpus_words'] = corpus_word_freq.most_common(20)
        
        # Statistiques g√©n√©rales
        total_words = len(all_text.split())
        valid_docs = sum(1 for a in all_analyses if a['is_valid'])
        
        corpus_analysis['total_words'] = total_words
        corpus_analysis['valid_documents'] = valid_docs
        corpus_analysis['total_documents'] = len(documents)
    
    progress_bar.empty()
    status_text.empty()
    
    return {
        'document_analyses': all_analyses,
        'document_stats': all_stats,
        'corpus_analysis': corpus_analysis,
        'all_text': all_text
    }

# ==================== FONCTIONS DE VISUALISATION ====================

def create_visualizations(data):
    """Cr√©e les visualisations pour le dashboard"""
    
    # Extraire les donn√©es
    stats_df = pd.DataFrame(data['document_stats'])
    corpus_analysis = data['corpus_analysis']
    
    visualizations = {}
    
    # 1. Graphique des documents par ann√©e
    if not stats_df.empty and 'Ann√©e' in stats_df.columns:
        try:
            # Nettoyer les ann√©es
            stats_df['Ann√©e_clean'] = stats_df['Ann√©e'].apply(
                lambda x: int(x) if str(x).isdigit() and 1900 <= int(x) <= 2024 else None
            )
            stats_df = stats_df.dropna(subset=['Ann√©e_clean'])
            
            year_counts = stats_df['Ann√©e_clean'].value_counts().sort_index()
            
            fig_years = px.bar(
                x=year_counts.index.astype(str),
                y=year_counts.values,
                title="üìÖ Documents par ann√©e",
                labels={'x': 'Ann√©e', 'y': 'Nombre de documents'},
                color=year_counts.values,
                color_continuous_scale='Blues'
            )
            fig_years.update_layout(xaxis_tickangle=-45)
            visualizations['years_chart'] = fig_years
        except:
            pass
    
    # 2. Graphique par type de document
    if not stats_df.empty and 'Type' in stats_df.columns:
        type_counts = stats_df['Type'].value_counts()
        fig_types = px.pie(
            values=type_counts.values,
            names=type_counts.index,
            title="üìã R√©partition par type de document",
            hole=0.3
        )
        fig_types.update_traces(textposition='inside', textinfo='percent+label')
        visualizations['types_chart'] = fig_types
    
    # 3. Graphique des th√®mes principaux
    if 'theme_frequencies' in corpus_analysis:
        theme_data = corpus_analysis['theme_frequencies']
        if theme_data:
            themes_df = pd.DataFrame({
                'Th√®me': list(theme_data.keys()),
                'Fr√©quence': list(theme_data.values())
            }).sort_values('Fr√©quence', ascending=False)
            
            fig_themes = px.bar(
                themes_df,
                x='Th√®me',
                y='Fr√©quence',
                title="üè∑Ô∏è Th√®mes principaux du corpus",
                color='Fr√©quence',
                color_continuous_scale='Viridis'
            )
            visualizations['themes_chart'] = fig_themes
    
    # 4. Mots les plus fr√©quents
    if 'top_corpus_words' in corpus_analysis:
        top_words = corpus_analysis['top_corpus_words'][:15]
        if top_words:
            words, counts = zip(*top_words)
            fig_words = px.bar(
                x=list(words),
                y=list(counts),
                title="üî§ Mots les plus fr√©quents (hors mots vides)",
                labels={'x': 'Mot', 'y': 'Fr√©quence'},
                color=list(counts),
                color_continuous_scale='thermal'
            )
            fig_words.update_layout(xaxis_tickangle=-45)
            visualizations['words_chart'] = fig_words
    
    # 5. Nuage de mots (simplifi√©)
    if 'top_corpus_words' in corpus_analysis:
        top_words = corpus_analysis['top_corpus_words'][:30]
        if top_words:
            words, counts = zip(*top_words)
            sizes = [c * 2 for c in counts]  # Ajuster la taille
            
            fig_cloud = go.Figure()
            
            # Positionner les mots al√©atoirement
            import random
            for word, size in zip(words, sizes):
                fig_cloud.add_trace(go.Scatter(
                    x=[random.random()],
                    y=[random.random()],
                    mode='text',
                    text=[word],
                    textfont=dict(size=size, color=f'rgb({random.randint(50,200)},{random.randint(50,200)},{random.randint(50,200)})'),
                    showlegend=False
                ))
            
            fig_cloud.update_layout(
                title="‚òÅÔ∏è Nuage de mots cl√©s",
                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                plot_bgcolor='white'
            )
            visualizations['wordcloud'] = fig_cloud
    
    return visualizations

# ==================== INTERFACE STREAMLIT ====================

# Initialisation de l'√©tat de session
if 'documents_data' not in st.session_state:
    st.session_state.documents_data = None
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'search_performed' not in st.session_state:
    st.session_state.search_performed = False
if 'visualizations' not in st.session_state:
    st.session_state.visualizations = None

# Sidebar
with st.sidebar:
    st.header("üîß Contr√¥les")
    
    # Mode de fonctionnement
    mode = st.radio(
        "Mode d'op√©ration",
        ["Scraping r√©el", "Donn√©es de d√©monstration", "Test unitaire"],
        help="Choisissez le mode de r√©cup√©ration des documents"
    )
    
    # Options de scraping
    st.subheader("Options de recherche")
    max_documents = st.slider("Nombre max de documents", 10, 100, 50)
    
    # Bouton d'action principal
    if st.button("üöÄ Lancer la recherche et l'analyse", type="primary", use_container_width=True):
        with st.spinner("Scraping et analyse en cours..."):
            try:
                # Recherche des documents
                if mode == "Scraping r√©el":
                    documents = search_bumidom_documents_real()
                elif mode == "Test unitaire":
                    documents = get_sample_documents()[:5]
                else:  # Mode d√©mo
                    documents = get_sample_documents()[:max_documents]
                
                if documents:
                    # Analyse des documents
                    analysis_results = analyze_all_documents(documents)
                    
                    # Cr√©er les visualisations
                    visualizations = create_visualizations(analysis_results)
                    
                    # Stocker dans la session
                    st.session_state.documents_data = analysis_results
                    st.session_state.visualizations = visualizations
                    st.session_state.analysis_done = True
                    st.session_state.search_performed = True
                    
                    st.success(f"‚úÖ Analyse termin√©e! {len(documents)} documents trait√©s.")
                else:
                    st.error("‚ùå Aucun document trouv√©.")
                    
            except Exception as e:
                st.error(f"‚ùå Erreur: {str(e)}")
    
    st.divider()
    
    # Gestion du cache
    st.header("üíæ Gestion des donn√©es")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üßπ Vider le cache", type="secondary"):
            if os.path.exists("pdf_cache"):
                import shutil
                shutil.rmtree("pdf_cache")
                st.success("Cache vid√©!")
                st.rerun()
    
    with col2:
        if st.button("üîÑ Rafra√Æchir", type="secondary"):
            st.rerun()
    
    # Informations
    if st.session_state.analysis_done:
        st.divider()
        st.header("üìä Statistiques")
        
        data = st.session_state.documents_data
        if data and 'corpus_analysis' in data:
            stats = data['corpus_analysis']
            
            st.metric("Documents totaux", stats.get('total_documents', 0))
            st.metric("Documents valides", stats.get('valid_documents', 0))
            st.metric("Mots analys√©s", f"{stats.get('total_words', 0):,}")
    
    st.divider()
    st.caption("Dashboard BUMIDOM v1.0 ‚Ä¢ Archives Assembl√©e Nationale")

# ==================== CONTENU PRINCIPAL ====================

if st.session_state.search_performed and st.session_state.analysis_done:
    data = st.session_state.documents_data
    visuals = st.session_state.visualizations
    
    # M√©triques principales
    st.header("üìà Vue d'ensemble")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_docs = len(data['document_stats'])
        st.metric("Documents", total_docs)
    
    with col2:
        valid_docs = sum(1 for doc in data['document_stats'] if doc['Valide'] == '‚úÖ')
        st.metric("Documents valides", valid_docs)
    
    with col3:
        total_pages = sum(doc['Pages'] for doc in data['document_stats'])
        st.metric("Pages totales", total_pages)
    
    with col4:
        if data['corpus_analysis']:
            total_words = data['corpus_analysis'].get('total_words', 0)
            st.metric("Mots analys√©s", f"{total_words:,}")
    
    # Tableau des documents
    st.header("üìÑ Documents analys√©s")
    
    stats_df = pd.DataFrame(data['document_stats'])
    
    # Filtres
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if not stats_df.empty and 'Ann√©e' in stats_df.columns:
            years = sorted([y for y in stats_df['Ann√©e'].unique() if str(y).isdigit()])
            selected_years = st.multiselect("Filtrer par ann√©e", years, default=years)
            if selected_years:
                stats_df = stats_df[stats_df['Ann√©e'].isin(selected_years)]
    
    with col2:
        if not stats_df.empty and 'Type' in stats_df.columns:
            types = sorted(stats_df['Type'].unique())
            selected_types = st.multiselect("Filtrer par type", types, default=types)
            if selected_types:
                stats_df = stats_df[stats_df['Type'].isin(selected_types)]
    
    with col3:
        if not stats_df.empty and 'Valide' in stats_df.columns:
            validity_filter = st.multiselect("√âtat", ['‚úÖ', '‚ùå'], default=['‚úÖ', '‚ùå'])
            if validity_filter:
                stats_df = stats_df[stats_df['Valide'].isin(validity_filter)]
    
    # Afficher le tableau
    st.dataframe(
        stats_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "Titre": st.column_config.TextColumn(width="large"),
            "URL": st.column_config.LinkColumn(display_text="Lien")
        }
    )
    
    # Visualisations
    st.header("üìä Visualisations interactives")
    
    if visuals:
        # Onglets pour les graphiques
        tab1, tab2, tab3, tab4 = st.tabs(["üìÖ Chronologie", "üìã Types", "üè∑Ô∏è Th√®mes", "üî§ Mots"])
        
        with tab1:
            if 'years_chart' in visuals:
                st.plotly_chart(visuals['years_chart'], use_container_width=True)
            else:
                st.info("Graphique chronologique non disponible")
        
        with tab2:
            if 'types_chart' in visuals:
                st.plotly_chart(visuals['types_chart'], use_container_width=True)
            else:
                st.info("Graphique des types non disponible")
        
        with tab3:
            if 'themes_chart' in visuals:
                st.plotly_chart(visuals['themes_chart'], use_container_width=True)
            else:
                st.info("Graphique des th√®mes non disponible")
        
        with tab4:
            col1, col2 = st.columns(2)
            with col1:
                if 'words_chart' in visuals:
                    st.plotly_chart(visuals['words_chart'], use_container_width=True)
            with col2:
                if 'wordcloud' in visuals:
                    st.plotly_chart(visuals['wordcloud'], use_container_width=True)
    
    # Analyse d√©taill√©e par document
    st.header("üîç Analyse document par document")
    
    for i, doc_analysis in enumerate(data['document_analyses']):
        # Appliquer les filtres
        if not stats_df.empty:
            if doc_analysis['title'][:80] not in stats_df['Titre'].values:
                continue
        
        with st.expander(f"{doc_analysis['title']} ({doc_analysis['date']}) - {doc_analysis['type']}"):
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Mots", doc_analysis['word_count'])
            
            with col2:
                st.metric("Pages", doc_analysis['pages'])
            
            with col3:
                bumidom_count = doc_analysis['keyword_counts'].get('BUMIDOM', 0)
                st.metric("BUMIDOM", bumidom_count)
            
            with col4:
                migrant_count = doc_analysis['keyword_counts'].get('migrant', 0) + doc_analysis['keyword_counts'].get('migrants', 0)
                st.metric("Migrants", migrant_count)
            
            # Aper√ßu du texte
            st.subheader("Extrait")
            st.text(doc_analysis['text_preview'])
            
            # Mots-cl√©s
            st.subheader("Mots-cl√©s principaux")
            if doc_analysis['keyword_counts']:
                keywords_df = pd.DataFrame(
                    list(doc_analysis['keyword_counts'].items()),
                    columns=['Mot-cl√©', 'Occurrences']
                ).sort_values('Occurrences', ascending=False)
                
                st.dataframe(keywords_df, use_container_width=True, hide_index=True)
            
            # Th√®mes d√©tect√©s
            st.subheader("Th√®mes d√©tect√©s")
            if doc_analysis['themes']:
                themes_df = pd.DataFrame(
                    list(doc_analysis['themes'].items()),
                    columns=['Th√®me', 'Occurrences']
                ).sort_values('Occurrences', ascending=False)
                
                st.dataframe(themes_df, use_container_width=True, hide_index=True)
    
    # Export des donn√©es
    st.header("üíæ Export des r√©sultats")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV
        csv_data = stats_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• T√©l√©charger CSV",
            data=csv_data,
            file_name="bumidom_analysis.csv",
            mime="text/csv",
            use_container_width=True
        )
    
    with col2:
        # Export JSON
        export_data = {
            'metadata': {
                'date_export': datetime.now().isoformat(),
                'total_documents': len(data['document_analyses']),
                'source': 'Archives Assembl√©e Nationale'
            },
            'documents': data['document_analyses']
        }
        
        json_data = json.dumps(export_data, ensure_ascii=False, indent=2)
        st.download_button(
            label="üì• T√©l√©charger JSON",
            data=json_data,
            file_name="bumidom_analysis.json",
            mime="application/json",
            use_container_width=True
        )
    
    with col3:
        # Rapport texte
        report_text = f"""RAPPORT D'ANALYSE BUMIDOM
{'='*50}

Date d'analyse: {datetime.now().strftime('%Y-%m-%d %H:%M')}
Documents analys√©s: {len(data['document_analyses'])}
Documents valides: {valid_docs}
Pages totales: {total_pages}
Mots analys√©s: {total_words:,}

TH√àMES PRINCIPAUX:
"""
        
        if data['corpus_analysis'].get('theme_frequencies'):
            for theme, freq in data['corpus_analysis']['theme_frequencies'].items():
                report_text += f"\n- {theme}: {freq} occurrences"
        
        report_text += "\n\nDOCUMENTS ANALYS√âS:\n"
        for doc in data['document_stats'][:20]:  # Limiter aux 20 premiers
            report_text += f"\n- {doc['Titre'][:60]}... ({doc['Ann√©e']}, {doc['Type']}, {doc['Pages']} pages)"
        
        if len(data['document_stats']) > 20:
            report_text += f"\n\n... et {len(data['document_stats']) - 20} autres documents"
        
        st.download_button(
            label="üìÑ Rapport texte",
            data=report_text.encode('utf-8'),
            file_name="rapport_bumidom.txt",
            mime="text/plain",
            use_container_width=True
        )

else:
    # √âcran d'accueil
    st.header("Bienvenue dans l'Analyseur BUMIDOM")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üéØ √Ä propos
        Cet outil analyse les documents parlementaires fran√ßais concernant le **BUMIDOM**, organisme ayant g√©r√© les migrations des DOM-TOM vers la m√©tropole entre 1963 et 1982.
        
        ### üîç Sources
        - Archives de l'Assembl√©e Nationale
        - Documents parlementaires
        - Rapports officiels
        - Comptes rendus de d√©bats
        """)
    
    with col2:
        st.markdown("""
        ### üöÄ Comment utiliser
        1. **Configurez** le mode dans la sidebar
        2. **Lancez** la recherche et l'analyse
        3. **Explorez** les r√©sultats via les visualisations
        4. **Exportez** les donn√©es pour vos recherches
        
        ### üìä Fonctionnalit√©s
        - Scraping automatique des archives
        - Extraction de texte depuis PDF
        - Analyse lexicale et th√©matique
        - Visualisations interactives
        - Export multi-formats
        """)
    
    st.divider()
    
    # Section technique
    with st.expander("üõ†Ô∏è Configuration technique"):
        st.markdown("""
        **D√©pendances Python:**
        ```bash
        pip install streamlit requests beautifulsoup4 pymupdf pandas plotly nltk
        ```
        
        **Fonctions principales:**
        1. `search_bumidom_documents_real()` - Scraping du site
        2. `extract_text_from_pdf_real()` - Extraction PDF
        3. `analyze_all_documents()` - Analyse textuelle
        4. `create_visualizations()` - G√©n√©ration de graphiques
        
        **Structure des donn√©es:**
        - Cache PDF local pour √©viter les re-t√©l√©chargements
        - Analyse NLTK pour le traitement du langage
        - Visualisations Plotly pour l'interactivit√©
        - Interface Streamlit responsive
        """)

# ==================== PIED DE PAGE ====================

st.divider()
st.markdown(
    """
    <div style='text-align: center; color: #666; font-size: 0.9em;'>
    Dashboard d'analyse BUMIDOM ‚Ä¢ Archives de l'Assembl√©e Nationale ‚Ä¢ 
    <a href='https://archives.assembl√©e-nationale.fr' target='_blank'>Source des donn√©es</a> ‚Ä¢ 
    Outil de recherche historique
    </div>
    """,
    unsafe_allow_html=True
)
