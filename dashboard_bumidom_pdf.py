import streamlit as st
import pandas as pd
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import re
import os

# ==================== CONFIGURATION ====================
st.set_page_config(page_title="Dashboard API Google CSE", layout="wide")
st.title("üîç Dashboard API - Archives Assembl√©e Nationale")
st.markdown("**Analyse COMPL√àTE de 131 r√©sultats BUMIDOM**")

# ==================== FONCTIONS DE PARSING SP√âCIFIQUES ====================

def parser_json_bumidom_complet(json_data):
    """Parser SP√âCIFIQUE pour le fichier JSON BUMIDOM avec 131 r√©sultats"""
    resultats = []
    
    try:
        # Votre fichier a une structure sp√©cifique
        if 'results' in json_data:
            items = json_data['results']
        elif isinstance(json_data, dict) and 'results' in list(json_data.values())[0]:
            # Structure wrapper
            items = list(json_data.values())[0]['results']
        else:
            # Chercher dans toute la structure
            for key, value in json_data.items():
                if isinstance(value, list) and len(value) > 0:
                    items = value
                    break
        
        st.info(f"‚úÖ {len(items)} r√©sultats trouv√©s dans la structure principale")
        
        # Parser CHAQUE r√©sultat individuellement
        for i, item in enumerate(items):
            try:
                # Extraire les informations selon votre structure sp√©cifique
                titre = item.get('title', item.get('titleNoFormatting', f'Document {i+1}'))
                url = item.get('url', item.get('unescapedUrl', item.get('link', '')))
                
                # Extraire le contenu (description)
                description = ""
                if 'contentNoFormatting' in item:
                    description = item['contentNoFormatting']
                elif 'content' in item:
                    description = item['content']
                elif 'snippet' in item:
                    description = item['snippet']
                
                # Nettoyer les entit√©s HTML
                if description:
                    description = description.replace('\\u003cb\\u003e', '').replace('\\u003c/b\\u003e', '')
                    description = description.replace('&#39;', "'").replace('&nbsp;', ' ')
                    description = description.replace('&quot;', '"')
                
                # Extraire la date depuis le contenu
                date_doc = "Date inconnue"
                if description:
                    # Chercher des patterns de date
                    date_patterns = [
                        r'(\d{1,2}\s+[a-z√©√ª]+\s+\d{4})',  # 26 oct. 1971
                        r'(\d{4})',                      # 1971
                        r'(\d{1,2}/\d{1,2}/\d{4})',      # 26/10/1971
                        r'(\d{1,2}\s+[a-zA-Z]+\s+\d{4})' # 26 October 1971
                    ]
                    
                    for pattern in date_patterns:
                        date_match = re.search(pattern, description, re.IGNORECASE)
                        if date_match:
                            date_doc = date_match.group(1)
                            break
                
                # D√©tecter le type de document
                type_doc = "Document"
                file_format = item.get('fileFormat', '')
                
                if '.pdf' in url.lower() or 'PDF' in file_format or 'pdf' in str(item).lower():
                    type_doc = "PDF"
                elif 'archives.assemblee-nationale.fr' in url:
                    if '/cri/' in url:
                        type_doc = "Compte rendu"
                    elif 'journal' in titre.lower() or 'JOURNAL' in titre or 'OFFICIEL' in titre:
                        type_doc = "Journal Officiel"
                    elif 'constitution' in titre.lower():
                        type_doc = "Constitution"
                    elif '/qst/' in url:
                        type_doc = "Question √©crite"
                
                # Extraire la l√©gislature
                legislature = ""
                
                # Chercher dans l'URL
                if url:
                    leg_match_url = re.search(r'/(\d+)/cri/', url)
                    if leg_match_url:
                        legislature = leg_match_url.group(1)
                    else:
                        leg_match_url = re.search(r'/(\d+)/qst/', url)
                        if leg_match_url:
                            legislature = leg_match_url.group(1)
                
                # Chercher dans le titre
                if not legislature and titre:
                    leg_match_title = re.search(r'(\d+)[\'¬∞]?\s+L√©gislature', titre)
                    if leg_match_title:
                        legislature = leg_match_title.group(1)
                
                # Extraire les ann√©es
                periode = "Inconnue"
                if url:
                    # Pattern: /1971-1972-ordonnaire1/
                    annee_match = re.search(r'/(\d{4})-(\d{4})', url)
                    if annee_match:
                        periode = f"{annee_match.group(1)}-{annee_match.group(2)}"
                
                if periode == "Inconnue" and description:
                    # Pattern: 1971-1972
                    annee_match = re.search(r'(\d{4})\s*-\s*(\d{4})', description)
                    if annee_match:
                        periode = f"{annee_match.group(1)}-{annee_match.group(2)}"
                    else:
                        # Pattern: 1971
                        annee_match = re.search(r'(\d{4})', date_doc)
                        if annee_match:
                            annee = annee_match.group(1)
                            periode = f"{annee}"
                
                # Score de pertinence (bas√© sur la position dans le JSON)
                score = 100 - (i * 0.5)  # Plus doux pour 131 r√©sultats
                
                # M√©tadonn√©es
                metadonnees = {}
                if 'richSnippet' in item:
                    metadonnees = item['richSnippet']
                if 'breadcrumbUrl' in item:
                    metadonnees['breadcrumbs'] = item['breadcrumbUrl'].get('crumbs', [])
                
                # Identifier le domaine
                visible_url = item.get('visibleUrl', '')
                if not visible_url and url:
                    from urllib.parse import urlparse
                    try:
                        parsed = urlparse(url)
                        visible_url = parsed.netloc
                    except:
                        visible_url = url[:50] + "..."
                
                # Ajouter au r√©sultat
                resultats.append({
                    'id': f"R{i+1:03d}",
                    'titre': titre[:150] + "..." if len(titre) > 150 else titre,
                    'url': url,
                    'description': description[:250] + "..." if description and len(description) > 250 else (description or "Pas de description"),
                    'type': type_doc,
                    'legislature': legislature,
                    'periode': periode,
                    'date_doc': date_doc,
                    'position': i + 1,
                    'score': score,
                    'format': file_format,
                    'visible_url': visible_url,
                    'metadonnees': json.dumps(metadonnees, ensure_ascii=False) if metadonnees else '',
                    'timestamp': datetime.now().isoformat(),
                    'doc_id': f"DOC_{i+1:04d}"
                })
                
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erreur sur l'√©l√©ment {i+1}: {str(e)[:100]}")
                continue
        
        st.success(f"üéâ {len(resultats)} r√©sultats pars√©s avec succ√®s!")
        return resultats
        
    except Exception as e:
        st.error(f"‚ùå Erreur majeure lors du parsing: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        return []

def charger_fichier_json_complet():
    """Charge le fichier JSON complet"""
    try:
        # Lire le fichier
        with open('json.txt', 'r', encoding='utf-8') as f:
            contenu = f.read()
        
        st.info(f"üìÅ Fichier charg√©: {len(contenu):,} caract√®res")
        
        # Essayer de parser directement
        try:
            data = json.loads(contenu)
            st.success("‚úÖ JSON pars√© directement")
            return data
        except json.JSONDecodeError:
            st.warning("‚ö†Ô∏è JSON direct √©chou√©, tentative de nettoyage...")
            
            # Nettoyer le JSON
            contenu_nettoye = nettoyer_json_bumidom(contenu)
            
            try:
                data = json.loads(contenu_nettoye)
                st.success("‚úÖ JSON nettoy√© et pars√©")
                return data
            except Exception as e:
                st.error(f"‚ùå √âchec du parsing m√™me apr√®s nettoyage: {e}")
                return None
                
    except FileNotFoundError:
        st.error("‚ùå Fichier 'json.txt' non trouv√©!")
        st.info("Placez votre fichier JSON complet dans le m√™me dossier que ce script")
        return None
    except Exception as e:
        st.error(f"‚ùå Erreur de chargement: {str(e)}")
        return None

def nettoyer_json_bumidom(contenu):
    """Nettoie sp√©cifiquement le JSON BUMIDOM"""
    # Supprimer la fonction wrapper
    contenu = re.sub(r'google\.search\.cse\.api\d+\(\s*', '', contenu)
    contenu = re.sub(r'\);\s*$', '', contenu)
    
    # Remplacer les simples quotes par des doubles quotes pour les cl√©s JSON
    lines = contenu.split('\n')
    cleaned_lines = []
    
    for line in lines:
        # Remplacer les cl√©s avec simples quotes
        line = re.sub(r'(\s*)(\w+)(\s*):(\s*)\'', r'\1"\2"\3:\4"', line)
        line = re.sub(r'\'(,?)\s*$', r'"\1', line)
        line = line.replace("' : '", '" : "')
        line = line.replace("': '", '": "')
        line = line.replace("',", '",')
        
        # G√©rer les apostrophes dans le contenu
        line = line.replace("\\'", "'")
        line = line.replace("&#39;", "'")
        
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

# ==================== INTERFACE PRINCIPALE ====================

# Initialisation
if 'donnees_completes' not in st.session_state:
    st.session_state.donnees_completes = []
if 'json_brut' not in st.session_state:
    st.session_state.json_brut = None

# Sidebar
with st.sidebar:
    st.header("‚öôÔ∏è Configuration BUMIDOM")
    
    # Bouton d'analyse principal
    if st.button("üöÄ ANALYSER LES 131 R√âSULTATS BUMIDOM", 
                 type="primary", 
                 use_container_width=True,
                 help="Cliquez pour analyser TOUS les r√©sultats du fichier JSON"):
        
        with st.spinner("Chargement et analyse compl√®te en cours..."):
            # Charger le JSON
            json_data = charger_fichier_json_complet()
            
            if json_data:
                st.session_state.json_brut = json_data
                
                # Parser TOUS les r√©sultats
                resultats = parser_json_bumidom_complet(json_data)
                st.session_state.donnees_completes = resultats
                
                # Afficher les statistiques
                if resultats:
                    st.success(f"‚úÖ {len(resultats)} r√©sultats analys√©s!")
                    
                    # Statistiques par type
                    types = {}
                    for r in resultats:
                        types[r['type']] = types.get(r['type'], 0) + 1
                    
                    st.write("**üìä R√©partition:**")
                    for type_name, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
                        st.write(f"- {type_name}: {count}")
            else:
                st.error("‚ùå Impossible de charger le fichier JSON")
    
    # Statistiques
    if st.session_state.donnees_completes:
        st.divider()
        st.subheader("üìà Statistiques")
        
        total = len(st.session_state.donnees_completes)
        st.metric("R√©sultats totaux", total)
        
        # Types uniques
        types_uniques = len(set([r['type'] for r in st.session_state.donnees_completes]))
        st.metric("Types de documents", types_uniques)
        
        # L√©gislatures uniques
        legislatures_uniques = len(set([r['legislature'] for r in st.session_state.donnees_completes if r['legislature']]))
        st.metric("L√©gislatures", legislatures_uniques)

# ==================== CONTENU PRINCIPAL ====================

if st.session_state.donnees_completes:
    donnees = st.session_state.donnees_completes
    df = pd.DataFrame(donnees)
    
    # Header avec statistiques
    st.header(f"üìä Analyse compl√®te: {len(df)} documents BUMIDOM")
    
    # M√©triques principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        pdf_count = df[df['format'].str.contains('PDF', na=False)].shape[0]
        st.metric("Documents PDF", pdf_count)
    
    with col2:
        cr_count = df[df['type'] == 'Compte rendu'].shape[0]
        st.metric("Comptes rendus", cr_count)
    
    with col3:
        jo_count = df[df['type'] == 'Journal Officiel'].shape[0]
        st.metric("Journaux Officiels", jo_count)
    
    with col4:
        qst_count = df[df['type'] == 'Question √©crite'].shape[0]
        st.metric("Questions √©crites", qst_count)
    
    # ==================== TABLEAU COMPLET ====================
    st.subheader("üìã Liste compl√®te des documents")
    
    # Filtres rapides
    with st.expander("üîç Filtres rapides", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Filtre par type
            types_disponibles = sorted(df['type'].unique())
            types_selection = st.multiselect(
                "Types de documents",
                types_disponibles,
                default=types_disponibles
            )
        
        with col2:
            # Filtre par l√©gislature
            legislatures_disponibles = sorted([l for l in df['legislature'].unique() if l])
            leg_selection = st.multiselect(
                "L√©gislatures",
                legislatures_disponibles,
                default=legislatures_disponibles
            )
        
        with col3:
            # Filtre par p√©riode
            periodes_disponibles = sorted([p for p in df['periode'].unique() if p != "Inconnue"])
            periode_selection = st.multiselect(
                "P√©riodes",
                periodes_disponibles,
                default=periodes_disponibles[:10] if len(periodes_disponibles) > 10 else periodes_disponibles
            )
    
    # Appliquer les filtres
    df_filtre = df[
        (df['type'].isin(types_selection)) &
        (df['legislature'].isin(leg_selection) if leg_selection else True) &
        (df['periode'].isin(periode_selection) if periode_selection else True)
    ]
    
    st.info(f"üìã Affichage de {len(df_filtre)} sur {len(df)} documents ({len(df_filtre)/len(df)*100:.1f}%)")
    
    # Options d'affichage
    col1, col2 = st.columns(2)
    with col1:
        items_per_page = st.selectbox("R√©sultats par page", [10, 25, 50, 100, 200], index=2)
    with col2:
        tri_par = st.selectbox("Trier par", ['position', 'score', 'periode', 'legislature', 'type'], index=0)
        ordre_tri = st.selectbox("Ordre", ['ascendant', 'descendant'], index=1)
    
    # Tri
    df_filtre_trie = df_filtre.sort_values(
        tri_par,
        ascending=(ordre_tri == 'ascendant')
    )
    
    # Pagination
    total_pages = max(1, (len(df_filtre_trie) + items_per_page - 1) // items_per_page)
    page = st.number_input("Page", min_value=1, max_value=total_pages, value=1)
    start_idx = (page - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, len(df_filtre_trie))
    
    df_page = df_filtre_trie.iloc[start_idx:end_idx]
    
    st.write(f"**Page {page}/{total_pages}** ({start_idx+1}-{end_idx} sur {len(df_filtre_trie)})")
    
    # Affichage du tableau
    st.dataframe(
        df_page[[
            'id', 'titre', 'type', 'legislature', 
            'periode', 'date_doc', 'score', 'visible_url'
        ]],
        use_container_width=True,
        hide_index=True,
        column_config={
            "id": st.column_config.TextColumn("ID", width="small"),
            "titre": st.column_config.TextColumn("Titre", width="large"),
            "type": st.column_config.TextColumn("Type"),
            "legislature": st.column_config.TextColumn("L√©gislature"),
            "periode": st.column_config.TextColumn("P√©riode"),
            "date_doc": st.column_config.TextColumn("Date"),
            "score": st.column_config.NumberColumn("Score", format="%d"),
            "visible_url": st.column_config.TextColumn("Source", width="medium")
        }
    )
    
    # ==================== VISUALISATIONS ====================
    st.subheader("üìà Visualisations")
    
    tab1, tab2, tab3, tab4 = st.tabs(["Par type", "Par p√©riode", "Par l√©gislature", "Scores"])
    
    with tab1:
        # Distribution par type
        type_counts = df_filtre['type'].value_counts()
        fig = px.pie(
            values=type_counts.values,
            names=type_counts.index,
            title=f"R√©partition par type ({len(type_counts)} types)",
            hole=0.3
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Distribution par p√©riode
        periode_counts = df_filtre['periode'].value_counts().head(20)
        fig = px.bar(
            x=periode_counts.index,
            y=periode_counts.values,
            title=f"Top 20 des p√©riodes",
            labels={'x': 'P√©riode', 'y': 'Documents'},
            color=periode_counts.values,
            color_continuous_scale='Viridis'
        )
        fig.update_layout(xaxis_tickangle=-45, height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        # Distribution par l√©gislature
        leg_counts = df_filtre[df_filtre['legislature'] != '']['legislature'].value_counts()
        if len(leg_counts) > 0:
            fig = px.bar(
                x=leg_counts.index,
                y=leg_counts.values,
                title="Documents par l√©gislature",
                labels={'x': 'L√©gislature', 'y': 'Documents'},
                color=leg_counts.values,
                color_continuous_scale='Blues'
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Aucune l√©gislure trouv√©e dans les donn√©es filtr√©es")
    
    with tab4:
        # Distribution des scores
        fig = px.histogram(
            df_filtre,
            x='score',
            nbins=20,
            title="Distribution des scores de pertinence",
            labels={'score': 'Score'},
            color_discrete_sequence=['#FF6B6B']
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    # ==================== D√âTAILS PAR DOCUMENT ====================
    st.subheader("üîç D√©tail document par document")
    
    if not df_filtre.empty:
        # S√©lection d'un document
        options = [(row['id'], f"{row['id']} - {row['titre'][:80]}... [{row['type']}]") 
                  for _, row in df_filtre.iterrows()]
        
        selected_id = st.selectbox(
            "Choisir un document √† inspecter",
            options=[opt[0] for opt in options],
            format_func=lambda x: dict(options).get(x, x)
        )
        
        if selected_id:
            doc = df_filtre[df_filtre['id'] == selected_id].iloc[0]
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.markdown(f"### üìÑ {doc['titre']}")
                
                if doc['description'] and doc['description'] != "Pas de description":
                    with st.expander("üìù Description compl√®te", expanded=True):
                        st.write(doc['description'])
                
                # URL
                if doc['url']:
                    st.markdown("**üîó Lien original:**")
                    st.code(doc['url'])
                    
                    # Bouton pour ouvrir
                    st.markdown(
                        f'<a href="{doc["url"]}" target="_blank" style="text-decoration: none;">'
                        '<button style="padding: 10px 20px; background-color: #4CAF50; color: white; '
                        'border: none; border-radius: 5px; cursor: pointer; margin: 5px 0;">'
                        'üìÑ Ouvrir le document PDF</button></a>',
                        unsafe_allow_html=True
                    )
            
            with col2:
                st.markdown("**üìä M√©tadonn√©es**")
                
                info_cols = st.columns(2)
                with info_cols[0]:
                    st.metric("Type", doc['type'])
                    st.metric("Position", doc['position'])
                    st.metric("Score", f"{doc['score']:.1f}")
                
                with info_cols[1]:
                    st.metric("L√©gislature", doc['legislature'] or "N/A")
                    st.metric("P√©riode", doc['periode'])
                    st.metric("Date", doc['date_doc'])
                
                # Informations techniques
                with st.expander("‚öôÔ∏è Techniques"):
                    st.write(f"**Format:** {doc['format']}")
                    st.write(f"**Source:** {doc['visible_url']}")
                    st.write(f"**ID technique:** {doc['doc_id']}")
                    st.write(f"**Extrait le:** {doc['timestamp'][:19]}")
    
    # ==================== EXPORT ====================
    st.subheader("üíæ Export des donn√©es")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV complet
        csv_complet = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="üì• CSV COMPLET",
            data=csv_complet,
            file_name=f"bumidom_complet_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv",
            use_container_width=True
        )
    
    with col2:
        # Export JSON structur√©
        json_struct = json.dumps(donnees, ensure_ascii=False, indent=2)
        st.download_button(
            label="üì• JSON STRUCTUR√â",
            data=json_struct.encode('utf-8'),
            file_name=f"bumidom_structur√©_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
            mime="application/json",
            use_container_width=True
        )
    
    with col3:
        # Export URLs seulement
        urls = "\n".join([d['url'] for d in donnees if d['url']])
        st.download_button(
            label="üîó LISTE DES URLs",
            data=urls.encode('utf-8'),
            file_name=f"urls_bumidom_{datetime.now().strftime('%Y%m%d_%H%M')}.txt",
            mime="text/plain",
            use_container_width=True
        )
    
    # ==================== DONN√âES BRUTES ====================
    with st.expander("üìä DONN√âES BRUTES (extrait)", expanded=False):
        if st.session_state.json_brut:
            # Afficher un extrait des donn√©es brutes
            if 'results' in st.session_state.json_brut:
                st.json(st.session_state.json_brut['results'][:2])  # 2 premiers r√©sultats
            elif isinstance(st.session_state.json_brut, list):
                st.json(st.session_state.json_brut[:2])
            else:
                st.json({k: st.session_state.json_brut[k] for k in list(st.session_state.json_brut.keys())[:2]})

else:
    # ==================== √âCRAN D'ACCUEIL ====================
    st.header("üìä Analyseur des 131 documents BUMIDOM")
    
    st.success("""
    ### ‚úÖ PR√äT √Ä ANALYSER VOS 131 DOCUMENTS
    
    **Votre fichier contient les donn√©es r√©elles de l'API Google CSE.**
    
    ### üéØ CE QUE CETTE VERSION FAIT DIFF√âREMMENT:
    1. **Analyse TOUS les 131 r√©sultats** de votre fichier JSON
    2. **Parser SP√âCIFIQUE** pour votre structure de donn√©es
    3. **Extraction COMPL√àTE** des m√©tadonn√©es
    4. **Interface OPTIMIS√âE** pour 100+ r√©sultats
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üìã CE QUI SERA EXTRACT:
        - **Tous les 131 r√©sultats** du fichier
        - **M√©tadonn√©es compl√®tes** de chaque document
        - **Informations techniques** (format, score, etc.)
        - **Donn√©es de contexte** (l√©gislature, p√©riode)
        """)
    
    with col2:
        st.markdown("""
        ### üöÄ COMMENT PROC√âDER:
        1. **Assurez-vous** que `json.txt` est dans le bon dossier
        2. **Cliquez** sur le bouton dans la sidebar
        3. **Attendez** l'analyse compl√®te
        4. **Explorez** TOUS les r√©sultats
        
        ### ‚è±Ô∏è TEMPS ESTIM√â:
        - Chargement: 2-3 secondes
        - Parsing: 3-5 secondes
        - Total: < 10 secondes
        """)
    
    # Instructions techniques
    with st.expander("üîß INFORMATIONS TECHNIQUES", expanded=False):
        st.markdown("""
        ### Structure attendue du fichier:
        ```
        {
          "context": {...},
          "results": [
            {
              "title": "...",
              "url": "...",
              "contentNoFormatting": "...",
              "fileFormat": "PDF/Adobe Acrobat",
              ...
            },
            ... 130 autres r√©sultats ...
          ]
        }
        ```
        
        ### Caract√©ristiques de vos donn√©es:
        - **131 r√©sultats** dans le tableau `results`
        - **Documents PDF** des archives de l'Assembl√©e Nationale
        - **Recherche sur BUMIDOM** (Bureau des migrations DOM)
        - **P√©riode:** 1960s-1980s
        - **Source:** archives.assembl√©e-nationale.fr
        """)
    
    st.warning("""
    ‚ö†Ô∏è **IMPORTANT:** 
    Si vous ne voyez que 13 r√©sultats avec l'ancienne version, 
    c'est parce qu'elle ne lisait pas correctement votre structure JSON.
    
    Cette nouvelle version est SP√âCIFIQUE √† votre fichier.
    """)

# ==================== PIED DE PAGE ====================
st.divider()
st.markdown(
    """
    <div style='text-align: center; color: #666; font-size: 0.9em;'>
    Dashboard sp√©cifique BUMIDOM ‚Ä¢ 131 documents analys√©s ‚Ä¢ 
    <span id='date'></span>
    <script>
        document.getElementById('date').innerHTML = new Date().toLocaleDateString('fr-FR');
    </script>
    </div>
    """,
    unsafe_allow_html=True
)
