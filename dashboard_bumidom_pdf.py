import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import fitz  # PyMuPDF pour les PDF
import io
import base64
import re
from datetime import datetime
import time
import urllib.parse
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import json

# Configuration de la page
st.set_page_config(
    page_title="Analyse Premium PDF - Archives AN",
    page_icon="üí∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Premium
st.markdown("""
<style>
    .premium-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    .metric-premium {
        background: white;
        border-radius: 15px;
        padding: 1.5rem;
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        border-left: 5px solid #764ba2;
        transition: transform 0.3s;
    }
    .metric-premium:hover {
        transform: translateY(-5px);
    }
    .pdf-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }
    .premium-button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 12px 24px;
        border-radius: 25px;
        font-weight: bold;
        cursor: pointer;
    }
</style>
""", unsafe_allow_html=True)

class PremiumPDFScraper:
    """Scraper premium pour extraire les 100 PDF des archives"""
    
    def __init__(self):
        self.base_url = "https://archives.assemblee-nationale.fr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        })
        self.pdf_data = []
        
    def search_google_custom_search(self, query="BUMIDOM", num_pages=10):
        """Utilise la recherche Google int√©gr√©e pour trouver des PDF"""
        
        st.info(f"üîç Recherche Google des PDF avec le terme: '{query}'")
        
        # Construction des URLs de recherche (simulation)
        search_urls = []
        for page in range(num_pages):
            # URL de recherche simul√©e bas√©e sur la structure du site
            search_url = f"{self.base_url}/recherche?q={query}&type=pdf&start={page*10}"
            search_urls.append(search_url)
        
        pdf_links = []
        
        # Recherche dans les pages de r√©sultats
        for url in search_urls:
            try:
                response = self.session.get(url, timeout=15)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Trouver tous les liens PDF
                pdf_elements = soup.find_all('a', href=re.compile(r'\.pdf$', re.I))
                
                for element in pdf_elements:
                    pdf_url = element.get('href', '')
                    if pdf_url:
                        if not pdf_url.startswith('http'):
                            pdf_url = urllib.parse.urljoin(self.base_url, pdf_url)
                        
                        title = element.get_text(strip=True) or element.get('title', '') or "Document sans titre"
                        
                        pdf_links.append({
                            'url': pdf_url,
                            'title': title[:200],  # Limiter la longueur
                            'source_url': url,
                            'rank': len(pdf_links) + 1
                        })
                
                # Recherche sp√©cifique BUMIDOM dans les textes
                bumidom_elements = soup.find_all(text=re.compile(r'bumidom|BUMIDOM', re.I))
                for element in bumidom_elements:
                    parent = element.parent
                    if parent.name == 'a' and parent.get('href', '').endswith('.pdf'):
                        pdf_url = parent.get('href')
                        if not pdf_url.startswith('http'):
                            pdf_url = urllib.parse.urljoin(self.base_url, pdf_url)
                        
                        pdf_links.append({
                            'url': pdf_url,
                            'title': f"BUMIDOM - {parent.get_text(strip=True)[:100]}",
                            'source_url': url,
                            'rank': len(pdf_links) + 1
                        })
                
                time.sleep(1)  # Respect du serveur
                
            except Exception as e:
                st.warning(f"Erreur page {url}: {str(e)[:100]}")
                continue
        
        return pdf_links[:100]  # Limiter √† 100 PDF
    
    def scrape_pdf_content(self, pdf_info):
        """T√©l√©charge et analyse un PDF"""
        try:
            st.write(f"üì• T√©l√©chargement: {pdf_info['title'][:50]}...")
            
            response = self.session.get(pdf_info['url'], timeout=30)
            
            if response.status_code == 200:
                # Analyser le PDF
                pdf_document = fitz.open(stream=response.content, filetype="pdf")
                
                # Extraire le texte
                full_text = ""
                metadata = pdf_document.metadata
                
                # Limiter aux premi√®res pages pour la performance
                for page_num in range(min(20, pdf_document.page_count)):
                    page = pdf_document[page_num]
                    full_text += page.get_text()
                
                # Analyse sp√©cifique
                analysis = self.analyze_pdf_content(full_text, pdf_info['title'])
                
                pdf_data = {
                    'titre': pdf_info['title'],
                    'url': pdf_info['url'],
                    'pages': pdf_document.page_count,
                    'taille_mo': len(response.content) / (1024 * 1024),
                    'texte_complet': full_text[:5000],  # Limit√© pour stockage
                    **analysis,
                    'metadata': metadata,
                    'date_extraction': datetime.now().isoformat(),
                    'score_pertinence': self.calculate_relevance_score(full_text, pdf_info['title'])
                }
                
                pdf_document.close()
                return pdf_data
                
        except Exception as e:
            st.error(f"‚ùå Erreur PDF {pdf_info['url']}: {str(e)[:100]}")
        
        return None
    
    def analyze_pdf_content(self, text, title):
        """Analyse avanc√©e du contenu PDF"""
        
        # D√©tection de termes cl√©s
        keywords_bumidom = ['bumidom', 'migration', 'outre-mer', 'dom', 'r√©paration', 'victimes']
        keywords_found = []
        
        for keyword in keywords_bumidom:
            if re.search(keyword, text, re.IGNORECASE):
                keywords_found.append(keyword)
        
        # Comptage des occurrences
        text_lower = text.lower()
        bumidom_count = len(re.findall(r'bumidom', text_lower))
        
        # Extraction de dates
        dates = re.findall(r'\d{2}/\d{2}/\d{4}', text)
        
        # D√©tection de noms de d√©put√©s
        deputes_pattern = r'(M\.|Mme|Monsieur|Madame)\s+[A-Z][a-z√©√®√™√´√†√¢√§√¥√∂√ª√º√ß]+\s+[A-Z][a-z√©√®√™√´√†√¢√§√¥√∂√ª√º√ß]+'
        deputes = re.findall(deputes_pattern, text)
        
        return {
            'mots_cles': keywords_found,
            'mentions_bumidom': bumidom_count,
            'dates_trouvees': dates[:10],  # Limiter √† 10 dates
            'deputes_mentionnes': list(set(deputes))[:5],
            'longueur_texte': len(text),
            'mots_uniques': len(set(text.lower().split())),
            'densite_bumidom': bumidom_count / max(1, len(text.split()) / 1000)
        }
    
    def calculate_relevance_score(self, text, title):
        """Calcule un score de pertinence pour le classement"""
        score = 0
        
        # Score bas√© sur le titre
        if re.search(r'bumidom', title, re.IGNORECASE):
            score += 50
        
        # Score bas√© sur le contenu
        text_lower = text.lower()
        bumidom_matches = len(re.findall(r'bumidom', text_lower))
        score += min(bumidom_matches * 10, 100)
        
        # Score bas√© sur la longueur (documents plus longs souvent plus d√©taill√©s)
        score += min(len(text) / 100, 50)
        
        return min(score, 100)
    
    def batch_scrape_pdfs(self, query="BUMIDOM", num_pdfs=100):
        """Scrape un lot de PDF"""
        
        with st.spinner(f"üîç Recherche de {num_pdfs} PDF..."):
            # √âtape 1: Recherche
            pdf_links = self.search_google_custom_search(query, num_pages=10)
            
            if not pdf_links:
                st.error("Aucun PDF trouv√©. V√©rifiez la connexion ou les param√®tres.")
                return []
            
            st.success(f"‚úÖ {len(pdf_links)} PDF trouv√©s")
            
            # √âtape 2: T√©l√©chargement et analyse
            progress_bar = st.progress(0)
            all_pdf_data = []
            
            for idx, pdf_info in enumerate(pdf_links[:num_pdfs]):
                # Mise √† jour de la progression
                progress = (idx + 1) / min(len(pdf_links), num_pdfs)
                progress_bar.progress(progress)
                
                # Analyse du PDF
                pdf_data = self.scrape_pdf_content(pdf_info)
                if pdf_data:
                    all_pdf_data.append(pdf_data)
                    st.write(f"‚úì Analys√©: {pdf_data['titre'][:60]}...")
                
                # Pause pour respecter le serveur
                time.sleep(0.5)
            
            progress_bar.empty()
            
            return all_pdf_data

class PremiumDashboard:
    """Dashboard premium avec fonctionnalit√©s avanc√©es"""
    
    def __init__(self):
        self.scraper = PremiumPDFScraper()
        self.pdf_data = []
        
    def display_premium_header(self):
        """En-t√™te premium du dashboard"""
        
        col1, col2, col3 = st.columns([2, 3, 1])
        
        with col1:
            st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/3/39/Logo_Assemblee_nationale_%28France%29.svg/800px-Logo_Assemblee_nationale_%28France%29.svg.png",
                    width=120)
        
        with col2:
            st.markdown("""
            <div class="premium-header">
                <h1>üí∞ ANALYSE PREMIUM BUMIDOM</h1>
                <h3>Archives de l'Assembl√©e Nationale - 100 PDF Analyse</h3>
                <p>Dashboard interactif avec scraping automatis√©, analyse IA et visualisations avanc√©es</p>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            st.markdown("""
            <div style="text-align: center; padding: 1rem;">
                <div style="font-size: 2rem;">üöÄ</div>
                <div style="font-weight: bold; color: #764ba2;">VERSION PREMIUM</div>
                <div style="font-size: 0.8rem; color: #666;">Analyse compl√®te</div>
            </div>
            """, unsafe_allow_html=True)
    
    def display_control_panel(self):
        """Panneau de contr√¥le premium"""
        
        st.sidebar.markdown("### üéõÔ∏è PANEL DE CONTR√îLE PREMIUM")
        
        # Recherche
        search_query = st.sidebar.text_input(
            "üîç Terme de recherche:",
            value="BUMIDOM migration outre-mer",
            help="Terme √† rechercher dans les PDF"
        )
        
        num_pdfs = st.sidebar.slider(
            "üìä Nombre de PDF √† analyser:",
            min_value=10,
            max_value=100,
            value=50,
            step=10
        )
        
        # Options avanc√©es
        st.sidebar.markdown("### ‚öôÔ∏è OPTIONS AVANC√âES")
        
        col_opt1, col_opt2 = st.sidebar.columns(2)
        
        with col_opt1:
            extract_full_text = st.checkbox("üìù Texte complet", value=True)
            analyze_sentiment = st.checkbox("üòä Analyse sentiment", value=True)
        
        with col_opt2:
            extract_tables = st.checkbox("üìä Extraire tables", value=False)
            detect_entities = st.checkbox("üë§ D√©tecter entit√©s", value=True)
        
        # Bouton d'analyse
        if st.sidebar.button("üöÄ Lancer l'analyse premium", type="primary", use_container_width=True):
            with st.spinner(f"Analyse de {num_pdfs} PDF en cours..."):
                self.pdf_data = self.scraper.batch_scrape_pdfs(search_query, num_pdfs)
                
                if self.pdf_data:
                    # Sauvegarde des donn√©es
                    self.save_analysis_data()
                    st.success(f"‚úÖ Analyse termin√©e: {len(self.pdf_data)} PDF analys√©s")
                    st.rerun()
                else:
                    st.error("‚ùå Aucun PDF n'a pu √™tre analys√©")
        
        # Statistiques rapides
        st.sidebar.markdown("---")
        st.sidebar.markdown("### üìà STATS RAPIDES")
        
        if self.pdf_data:
            df = pd.DataFrame(self.pdf_data)
            total_mentions = df['mentions_bumidom'].sum()
            avg_score = df['score_pertinence'].mean()
            
            st.sidebar.metric("üìÑ PDF Analys√©s", len(self.pdf_data))
            st.sidebar.metric("üîç Mentions BUMIDOM", f"{total_mentions:,}")
            st.sidebar.metric("‚≠ê Score moyen", f"{avg_score:.1f}/100")
        
        return search_query, num_pdfs
    
    def display_premium_metrics(self):
        """Affiche les m√©triques premium"""
        
        if not self.pdf_data:
            return
        
        df = pd.DataFrame(self.pdf_data)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown(f"""
            <div class="metric-premium">
                <div style="font-size: 2.5rem; color: #764ba2;">{len(df)}</div>
                <div style="color: #666;">üìÑ PDF Analys√©s</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            total_pages = df['pages'].sum()
            st.markdown(f"""
            <div class="metric-premium">
                <div style="font-size: 2.5rem; color: #764ba2;">{total_pages:,}</div>
                <div style="color: #666;">üìë Pages totales</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            total_mentions = df['mentions_bumidom'].sum()
            st.markdown(f"""
            <div class="metric-premium">
                <div style="font-size: 2.5rem; color: #764ba2;">{total_mentions:,}</div>
                <div style="color: #666;">üîç Mentions BUMIDOM</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            total_size = df['taille_mo'].sum()
            st.markdown(f"""
            <div class="metric-premium">
                <div style="font-size: 2.5rem; color: #764ba2;">{total_size:.1f}</div>
                <div style="color: #666;">üíæ Mo de donn√©es</div>
            </div>
            """, unsafe_allow_html=True)
    
    def display_pdf_explorer(self):
        """Explorateur de PDF premium"""
        
        st.markdown("### üìö EXPLORATEUR DE PDF PREMIUM")
        
        if not self.pdf_data:
            st.info("üéØ Lancez d'abord une analyse dans le panel de contr√¥le")
            return
        
        df = pd.DataFrame(self.pdf_data)
        
        # Filtres avanc√©s
        col_filt1, col_filt2, col_filt3, col_filt4 = st.columns(4)
        
        with col_filt1:
            min_score = st.slider("Score minimum", 0, 100, 50)
        
        with col_filt2:
            min_mentions = st.number_input("Mentions min", 0, 100, 1)
        
        with col_filt3:
            min_pages = st.number_input("Pages min", 1, 1000, 5)
        
        with col_filt4:
            sort_by = st.selectbox("Trier par", 
                                  ["Score pertinence", "Mentions BUMIDOM", "Pages", "Taille"])
        
        # Appliquer les filtres
        filtered_df = df.copy()
        filtered_df = filtered_df[filtered_df['score_pertinence'] >= min_score]
        filtered_df = filtered_df[filtered_df['mentions_bumidom'] >= min_mentions]
        filtered_df = filtered_df[filtered_df['pages'] >= min_pages]
        
        # Trier
        if sort_by == "Score pertinence":
            filtered_df = filtered_df.sort_values('score_pertinence', ascending=False)
        elif sort_by == "Mentions BUMIDOM":
            filtered_df = filtered_df.sort_values('mentions_bumidom', ascending=False)
        elif sort_by == "Pages":
            filtered_df = filtered_df.sort_values('pages', ascending=False)
        elif sort_by == "Taille":
            filtered_df = filtered_df.sort_values('taille_mo', ascending=False)
        
        # Afficher les PDF
        for idx, row in filtered_df.iterrows():
            with st.expander(f"üìÑ {row['titre'][:80]}...", expanded=False):
                col_pdf1, col_pdf2 = st.columns([3, 1])
                
                with col_pdf1:
                    # M√©triques du document
                    col_met1, col_met2, col_met3, col_met4 = st.columns(4)
                    
                    with col_met1:
                        st.metric("‚≠ê Score", f"{row['score_pertinence']:.1f}")
                    
                    with col_met2:
                        st.metric("üîç Mentions", row['mentions_bumidom'])
                    
                    with col_met3:
                        st.metric("üìë Pages", row['pages'])
                    
                    with col_met4:
                        st.metric("üíæ Taille", f"{row['taille_mo']:.1f} Mo")
                    
                    # Mots-cl√©s
                    if row['mots_cles']:
                        st.write("**üè∑Ô∏è Mots-cl√©s:**", ", ".join(row['mots_cles']))
                    
                    # Extraits du texte
                    if 'texte_complet' in row and row['texte_complet']:
                        with st.expander("üìù Voir extrait du texte"):
                            st.text(row['texte_complet'][:1000])
                
                with col_pdf2:
                    # Actions
                    st.markdown("**üîó Actions**")
                    
                    if st.button("üåê Ouvrir PDF", key=f"open_{idx}"):
                        st.markdown(f'<a href="{row["url"]}" target="_blank">Ouvrir dans un nouvel onglet</a>', 
                                  unsafe_allow_html=True)
                    
                    if st.button("üì• T√©l√©charger", key=f"dl_{idx}"):
                        self.download_pdf(row['url'], row['titre'])
                    
                    # Visualisation PDF
                    if st.button("üëÅÔ∏è Pr√©visualiser", key=f"preview_{idx}"):
                        self.display_pdf_preview(row['url'])
    
    def display_advanced_analytics(self):
        """Analyses avanc√©es"""
        
        if not self.pdf_data:
            return
        
        df = pd.DataFrame(self.pdf_data)
        
        st.markdown("### üìä ANALYTIQUES AVANC√âES")
        
        # Onglets d'analyse
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìà Distribution", 
            "üîç Corr√©lations", 
            "üìÖ √âvolution", 
            "‚òÅÔ∏è Mots-cl√©s"
        ])
        
        with tab1:
            # Histogramme des scores
            fig1 = px.histogram(
                df, 
                x='score_pertinence',
                nbins=20,
                title='Distribution des scores de pertinence',
                color_discrete_sequence=['#764ba2']
            )
            st.plotly_chart(fig1, use_container_width=True)
            
            # Box plot des mentions
            fig2 = px.box(
                df,
                y='mentions_bumidom',
                title='Distribution des mentions BUMIDOM',
                points='all'
            )
            st.plotly_chart(fig2, use_container_width=True)
        
        with tab2:
            # Scatter plot corr√©lations
            fig3 = px.scatter(
                df,
                x='pages',
                y='mentions_bumidom',
                size='taille_mo',
                color='score_pertinence',
                hover_name='titre',
                title='Corr√©lations: Pages vs Mentions',
                trendline='ols'
            )
            st.plotly_chart(fig3, use_container_width=True)
            
            # Matrice de corr√©lation
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            corr_matrix = df[numeric_cols].corr()
            
            fig4 = px.imshow(
                corr_matrix,
                text_auto=True,
                title='Matrice de corr√©lation',
                color_continuous_scale='RdBu'
            )
            st.plotly_chart(fig4, use_container_width=True)
        
        with tab3:
            # Analyse temporelle (si dates disponibles)
            if any(df['dates_trouvees'].apply(lambda x: len(x) > 0)):
                # Extraire les ann√©es
                all_years = []
                for dates in df['dates_trouvees']:
                    for date_str in dates:
                        try:
                            year = date_str.split('/')[-1]
                            all_years.append(int(year))
                        except:
                            pass
                
                if all_years:
                    year_counts = pd.Series(all_years).value_counts().sort_index()
                    
                    fig5 = px.line(
                        x=year_counts.index,
                        y=year_counts.values,
                        title='√âvolution temporelle des documents',
                        markers=True
                    )
                    fig5.update_layout(xaxis_title="Ann√©e", yaxis_title="Nombre de documents")
                    st.plotly_chart(fig5, use_container_width=True)
        
        with tab4:
            # Nuage de mots-cl√©s
            all_keywords = []
            for keywords in df['mots_cles']:
                all_keywords.extend(keywords)
            
            if all_keywords:
                word_freq = Counter(all_keywords)
                
                # Word cloud
                wordcloud = WordCloud(
                    width=800,
                    height=400,
                    background_color='white',
                    colormap='viridis'
                ).generate_from_frequencies(word_freq)
                
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                st.pyplot(fig)
                
                # Top mots-cl√©s
                top_keywords = pd.DataFrame(
                    word_freq.most_common(20),
                    columns=['Mot-cl√©', 'Fr√©quence']
                )
                
                fig6 = px.bar(
                    top_keywords,
                    x='Fr√©quence',
                    y='Mot-cl√©',
                    orientation='h',
                    title='Top 20 mots-cl√©s',
                    color='Fr√©quence'
                )
                st.plotly_chart(fig6, use_container_width=True)
    
    def display_ai_insights(self):
        """Insights g√©n√©r√©s par IA"""
        
        st.markdown("### ü§ñ INSIGHTS IA PREMIUM")
        
        if not self.pdf_data:
            return
        
        df = pd.DataFrame(self.pdf_data)
        
        # G√©n√©rer des insights automatiques
        insights = []
        
        # Insight 1: Top documents
        top_doc = df.loc[df['score_pertinence'].idxmax()]
        insights.append(f"üìÑ **Document le plus pertinent**: {top_doc['titre'][:60]}... (Score: {top_doc['score_pertinence']:.1f})")
        
        # Insight 2: Distribution
        avg_mentions = df['mentions_bumidom'].mean()
        insights.append(f"üîç **Moyenne mentions BUMIDOM**: {avg_mentions:.1f} par document")
        
        # Insight 3: Corr√©lation
        correlation = df['pages'].corr(df['mentions_bumidom'])
        if abs(correlation) > 0.3:
            insights.append(f"üìà **Corr√©lation pages-mentions**: {'positive' if correlation > 0 else 'negative'} ({correlation:.2f})")
        
        # Insight 4: Mots-cl√©s
        all_keywords = []
        for keywords in df['mots_cles']:
            all_keywords.extend(keywords)
        
        if all_keywords:
            most_common = Counter(all_keywords).most_common(1)[0]
            insights.append(f"üè∑Ô∏è **Mot-cl√© dominant**: '{most_common[0]}' ({most_common[1]} occurrences)")
        
        # Afficher les insights
        for insight in insights:
            st.info(insight)
        
        # Recommandations
        st.markdown("#### üí° RECOMMANDATIONS")
        
        col_rec1, col_rec2 = st.columns(2)
        
        with col_rec1:
            st.markdown("""
            **Pour approfondir:**
            1. √âtudier les documents avec score > 80
            2. Analyser les d√©bats parlementaires complets
            3. Rechercher les rapports d'enqu√™te
            """)
        
        with col_rec2:
            st.markdown("""
            **Pour la mon√©tisation:**
            1. Cr√©er des rapports premium
            2. Offrir des analyses personnalis√©es
            3. D√©velopper une API d'acc√®s aux donn√©es
            """)
    
    def display_export_premium(self):
        """Section d'export premium"""
        
        st.markdown("### üíæ EXPORT PREMIUM")
        
        col_exp1, col_exp2, col_exp3 = st.columns(3)
        
        with col_exp1:
            st.markdown("**üìä Donn√©es compl√®tes**")
            
            if st.button("üì• CSV Premium", use_container_width=True):
                self.export_csv()
            
            if st.button("üìà Excel Avanc√©", use_container_width=True):
                self.export_excel()
        
        with col_exp2:
            st.markdown("**üìÑ Rapports**")
            
            if st.button("üìã Rapport d'analyse", use_container_width=True):
                self.generate_report()
            
            if st.button("üìä Dashboard PDF", use_container_width=True):
                self.export_dashboard_pdf()
        
        with col_exp3:
            st.markdown("**üîó API & Int√©gration**")
            
            if st.button("üåê JSON API", use_container_width=True):
                self.export_json()
            
            if st.button("üîÑ Webhook", use_container_width=True):
                st.info("Configuration webhook disponible en version Entreprise")
    
    def save_analysis_data(self):
        """Sauvegarde les donn√©es d'analyse"""
        if self.pdf_data:
            df = pd.DataFrame(self.pdf_data)
            df.to_csv('pdf_analysis_premium.csv', index=False, encoding='utf-8-sig')
            df.to_json('pdf_analysis_premium.json', orient='records', force_ascii=False)
    
    def download_pdf(self, url, title):
        """T√©l√©charge un PDF"""
        try:
            response = requests.get(url)
            filename = re.sub(r'[^\w\-_\. ]', '_', title[:50]) + '.pdf'
            
            st.download_button(
                label="Cliquer pour t√©l√©charger",
                data=response.content,
                file_name=filename,
                mime="application/pdf"
            )
        except Exception as e:
            st.error(f"Erreur t√©l√©chargement: {str(e)}")
    
    def display_pdf_preview(self, url):
        """Affiche un aper√ßu PDF"""
        try:
            response = requests.get(url)
            base64_pdf = base64.b64encode(response.content).decode('utf-8')
            
            pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="600"></iframe>'
            st.markdown(pdf_display, unsafe_allow_html=True)
        except:
            st.warning("Aper√ßu non disponible pour ce PDF")
    
    def export_csv(self):
        """Export CSV"""
        if self.pdf_data:
            df = pd.DataFrame(self.pdf_data)
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            
            st.download_button(
                label="T√©l√©charger CSV",
                data=csv,
                file_name="bumidom_analysis_premium.csv",
                mime="text/csv"
            )
    
    def export_excel(self):
        """Export Excel"""
        if self.pdf_data:
            df = pd.DataFrame(self.pdf_data)
            
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='Analysis')
                
                # Ajouter des feuilles suppl√©mentaires
                summary = df.describe()
                summary.to_excel(writer, sheet_name='Summary')
            
            output.seek(0)
            
            st.download_button(
                label="T√©l√©charger Excel",
                data=output,
                file_name="bumidom_analysis_premium.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    
    def export_json(self):
        """Export JSON"""
        if self.pdf_data:
            json_str = json.dumps(self.pdf_data, ensure_ascii=False, indent=2)
            
            st.download_button(
                label="T√©l√©charger JSON",
                data=json_str,
                file_name="bumidom_analysis_premium.json",
                mime="application/json"
            )
    
    def generate_report(self):
        """G√©n√®re un rapport premium"""
        
        if not self.pdf_data:
            return
        
        df = pd.DataFrame(self.pdf_data)
        
        report = f"""
        RAPPORT PREMIUM D'ANALYSE BUMIDOM
        =================================
        
        G√©n√©r√© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        
        R√âSUM√â EX√âCUTIF
        ---------------
        ‚Ä¢ Documents analys√©s: {len(df)}
        ‚Ä¢ P√©riode couverte: Bas√©e sur les dates extraites
        ‚Ä¢ Score moyen de pertinence: {df['score_pertinence'].mean():.1f}/100
        
        ANALYSE QUANTITATIVE
        --------------------
        1. Volume de donn√©es:
           - Pages totales: {df['pages'].sum():,}
           - Donn√©es textuelles: {df['longueur_texte'].sum() / 1000000:.1f} millions de caract√®res
           - Taille totale PDF: {df['taille_mo'].sum():.1f} Mo
        
        2. R√©partition par score:
           - Excellent (80-100): {len(df[df['score_pertinence'] >= 80])} documents
           - Bon (60-79): {len(df[(df['score_pertinence'] >= 60) & (df['score_pertinence'] < 80)])} documents
           - Moyen (40-59): {len(df[(df['score_pertinence'] >= 40) & (df['score_pertinence'] < 60)])} documents
           - Faible (<40): {len(df[df['score_pertinence'] < 40])} documents
        
        3. Analyse th√©matique:
           - Mentions BUMIDOM totales: {df['mentions_bumidom'].sum():,}
           - Densit√© moyenne: {df['densite_bumidom'].mean():.3f} mentions/1000 mots
        
        DOCUMENTS CL√âS
        --------------
        """
        
        # Top 5 documents
        top_5 = df.nlargest(5, 'score_pertinence')
        for idx, row in top_5.iterrows():
            report += f"""
        {idx+1}. {row['titre'][:80]}...
           - Score: {row['score_pertinence']:.1f}
           - Mentions: {row['mentions_bumidom']}
           - Pages: {row['pages']}
           - Mots-cl√©s: {', '.join(row['mots_cles'][:5])}
            """
        
        report += f"""
        
        RECOMMANDATIONS STRAT√âGIQUES
        ----------------------------
        1. Prioriser l'analyse des {len(top_5)} documents top
        2. Approfondir les th√®mes r√©currents
        3. √âtablir une veille parlementaire continue
        
        M√âTHODOLOGIE
        ------------
        ‚Ä¢ Source: Archives de l'Assembl√©e Nationale
        ‚Ä¢ Outil: Dashboard Streamlit Premium
        ‚Ä¢ P√©riode d'analyse: {datetime.now().strftime('%B %Y')}
        ‚Ä¢ Algorithmes: Recherche s√©mantique, analyse de pertinence, extraction de motifs
        
        --- FIN DU RAPPORT ---
        """
        
        st.download_button(
            label="üì• T√©l√©charger le rapport",
            data=report,
            file_name="rapport_premium_bumidom.txt",
            mime="text/plain"
        )
    
    def export_dashboard_pdf(self):
        """Export du dashboard en PDF"""
        st.info("Fonctionnalit√© PDF export - Version Entreprise")
        st.markdown("""
        **Fonctionnalit√©s PDF premium:**
        - Export des visualisations haute r√©solution
        - Mise en page professionnelle
        - Pieds de page et en-t√™tes personnalis√©s
        - Chiffrement et protection des documents
        """)
    
    def run_dashboard(self):
        """Ex√©cute le dashboard complet"""
        
        # En-t√™te
        self.display_premium_header()
        
        # Panel de contr√¥le
        search_query, num_pdfs = self.display_control_panel()
        
        # M√©triques
        self.display_premium_metrics()
        
        # Onglets principaux
        tab_main1, tab_main2, tab_main3, tab_main4 = st.tabs([
            "üìö Explorateur", 
            "üìä Analytiques", 
            "ü§ñ Insights IA", 
            "üíæ Export"
        ])
        
        with tab_main1:
            self.display_pdf_explorer()
        
        with tab_main2:
            self.display_advanced_analytics()
        
        with tab_main3:
            self.display_ai_insights()
        
        with tab_main4:
            self.display_export_premium()

# Point d'entr√©e
if __name__ == "__main__":
    # Initialisation
    dashboard = PremiumDashboard()
    
    # Ex√©cution
    dashboard.run_dashboard()
